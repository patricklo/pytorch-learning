{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data and Extract DOI Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:52:58.905528Z",
     "iopub.status.busy": "2025-08-29T07:52:58.905286Z",
     "iopub.status.idle": "2025-08-29T07:53:16.657301Z",
     "shell.execute_reply": "2025-08-29T07:53:16.65609Z",
     "shell.execute_reply.started": "2025-08-29T07:52:58.905506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n",
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:16.659973Z",
     "iopub.status.busy": "2025-08-29T07:53:16.659322Z",
     "iopub.status.idle": "2025-08-29T07:53:31.798229Z",
     "shell.execute_reply": "2025-08-29T07:53:31.797294Z",
     "shell.execute_reply.started": "2025-08-29T07:53:16.659939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "LOCAL = sum(['KAGGLE' in k for k in os.environ]) == 0\n",
    "if not LOCAL:\n",
    "    # !mkdir -p /root/.cache/datalab/models/\n",
    "    # !mkdir -p /usr/local/lib/python3.11/dist-packages/static/fonts/\n",
    "    # !cp -r /kaggle/input/marker-models/marker_models/* /root/.cache/datalab/models/\n",
    "    # !cp /kaggle/input/marker-models/GoNotoCurrent-Regular.ttf /usr/local/lib/python3.11/dist-packages/static/fonts/\n",
    "    import sys\n",
    "    sys.path.append('/kaggle/input/mdc-tools-v2')\n",
    "from utils import *\n",
    "from bert_inference import *\n",
    "from xml_prase import *\n",
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "if LOCAL:   \n",
    "    labels_dir = 'train_labels.csv'\n",
    "else:\n",
    "    labels_dir =  \"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:31.79966Z",
     "iopub.status.busy": "2025-08-29T07:53:31.79926Z",
     "iopub.status.idle": "2025-08-29T07:53:44.670726Z",
     "shell.execute_reply": "2025-08-29T07:53:44.669809Z",
     "shell.execute_reply.started": "2025-08-29T07:53:31.79964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import pickle\n",
    "import vllm\n",
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "# Step 1: Read all PDFs and convert to text\n",
    "if LOCAL:\n",
    "    pdf_directory = \"/root/autodl-tmp/train/PDF\"\n",
    "    xml_directory = \"/root/autodl-tmp/train/XML\"\n",
    "    md_directory = \"./train_parsed/\"\n",
    "    unique_datasets = \"./unique_datasets.txt\"\n",
    "    reorganized_publication_dataset = \"./reorganized_publication_dataset.json\"\n",
    "else:\n",
    "    pdf_directory = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\" \\\n",
    "                    if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "                    else \"/kaggle/input/make-data-count-finding-data-references/train/PDF\"\n",
    "    xml_directory = \"/kaggle/input/make-data-count-finding-data-references/test/XML\" \\\n",
    "                    if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "                    else \"/kaggle/input/make-data-count-finding-data-references/train/XML\"\n",
    "    md_directory = \"/kaggle/working/pdf_parsed\"\n",
    "    unique_datasets = \"/kaggle/input/data-doi-and-accesion-ids/unique_datasets.txt\"\n",
    "    reorganized_publication_dataset = \"/kaggle/input/data-doi-and-accesion-ids/reorganized_publication_dataset.json\" ##D:\\Workspace\\GitHub\\Pytorch\\kaggle\\reorganized_publication_dataset.json\n",
    "\n",
    "patterns_to_find = {\n",
    "    # 1. 文献与书籍\n",
    "    'doi': re.compile(r'(10\\.\\d{4,}/(?:[-._/A-Z0-9]*(?:\\([-._/A-Z0-9]*\\)[-._/A-Z0-9]*)*[A-Z0-9]|[-._/A-Z0-9]*[A-Z0-9]))', re.IGNORECASE),\n",
    "    'doi2': re.compile(r'dryad\\.[^\\s\"<>]+|pasta\\/[^\\s\"<>]+|zenodo\\.\\d+|pangaea\\.\\d+', re.IGNORECASE),\n",
    "    'doi_ref': re.compile(r'(10\\.\\d{4,}/(?:[-._/A-Z0-9]*(?:\\([-._/A-Z0-9]*\\)[-._/A-Z0-9]*)*[A-Z0-9]|[-._/A-Z0-9]*[A-Z0-9]))', re.IGNORECASE),\n",
    "    'doi_ref2': re.compile(r'dryad\\.[^\\s\"<>]+|pasta\\/[^\\s\"<>]+|zenodo\\.\\d+|pangaea\\.\\d+', re.IGNORECASE),\n",
    "    \n",
    "    # ArrayExpress accession ID\n",
    "    'arrayexpress_id': re.compile(r'\\b(E-[A-Z]{4}-\\d+)\\b'),\n",
    "\n",
    "    # AlphaFold DB identifiers\n",
    "    'alphafold_id_1': re.compile(r'\\b(AF-[OPQ]\\d[A-Z0-9]{3}\\d+-F\\d)\\b', re.IGNORECASE),\n",
    "    'alphafold_id_2': re.compile(r'\\b(AF-[A-NR-Z]\\d(?:[A-Z][A-Z0-9]{2}\\d)+-F\\d)\\b', re.IGNORECASE),\n",
    "\n",
    "    # # BRENDA enzyme database identifiers\n",
    "    'brenda_ec_number': re.compile(r'\\b(\\d+\\.(?:\\d+|\\-)\\.(?:\\d+|\\-)\\.(?:\\d+|\\-))\\b'),\n",
    "    'brenda_tissue_ontology_id': re.compile(r'\\b(BTO: ?\\d{7})\\b', re.IGNORECASE),\n",
    "\n",
    "    # BioImage Archive accession ID\n",
    "    'bia_id': re.compile(r'\\b(S-BIAD\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # BioModels identifiers\n",
    "    'biomodels_id': re.compile(r'\\b((?:BIOMD|MODEL)\\d{10}|BMID\\d{12})\\b', re.IGNORECASE),\n",
    "\n",
    "    # BioSample accession ID\n",
    "    'biosample_id': re.compile(r'\\b(SAM[NED][A-Z]?\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # CATH protein structure classification identifiers\n",
    "    'cath_id': re.compile(r'\\b(\\d[a-zA-Z0-9]{4}\\d{2})\\b'),\n",
    "    'cath_domain': re.compile(r'\\b([1-4](?:\\.\\d+){3})\\b'),\n",
    "    \n",
    "    # Cellosaurus cell line accession ID\n",
    "    'cellosaurus_id': re.compile(r'\\b(CVCL_[a-zA-Z0-9]{4})\\b', re.IGNORECASE),\n",
    "\n",
    "    # ChEBI (Chemical Entities of Biological Interest) ID\n",
    "    'chebi_id': re.compile(r'\\b(CHEBI:\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # ChEMBL molecule ID\n",
    "    'chembl_id': re.compile(r'\\b(CHEMBL\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # Complex Portal accession ID\n",
    "    'complexportal_id': re.compile(r'\\b(CPX-\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # EBI Metagenomics sample ID\n",
    "    'metagenomics_sample_id': re.compile(r'\\b(SRS\\d{6})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Experimental Factor Ontology (EFO) ID\n",
    "    # 'efo_id': re.compile(r'\\b(EFO[:_]\\d+)\\b'),\n",
    "\n",
    "    # European Genome-phenome Archive (EGA) identifiers\n",
    "    'ega_id': re.compile(r'\\b(EGA[SDC]\\d{11})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Electron Microscopy Data Bank (EMDB) ID\n",
    "    'emdb_id': re.compile(r'\\b(EMD-\\d{4})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Electron Microscopy Public Image Archive (EMPIAR) ID\n",
    "    'empiar_id': re.compile(r'\\b(EMPIAR-\\d{5})\\b'),\n",
    "\n",
    "    # ENA/GenBank/DDBJ Accession Numbers\n",
    "    'ena_accession': re.compile(r'\\b([A-Z]\\d{5}|[A-Z]{2}\\d{6}|[A-RT-Z][A-Z]{3}S?\\d{8,9}|[A-Z]{3}\\d{5})\\b', re.IGNORECASE),\n",
    "    'sra_id': re.compile(r'\\b((?:[EDS]R[PXRAZ]|ERS)\\d{5,})\\b', re.IGNORECASE),\n",
    "    'ena_trace': re.compile(r'\\b(TI\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # Ensembl identifiers (Gene, Transcript, Protein)\n",
    "    'ensembl_id': re.compile(r'\\b(ENS[A-Z]*[GTP]\\d{11,})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Gene Ontology (GO) ID\n",
    "    # 'go_id': re.compile(r'\\b(GO:\\d{7})\\b', re.IGNORECASE),\n",
    "\n",
    "    # HGNC (HUGO Gene Nomenclature Committee) ID\n",
    "    # 'hgnc_id': re.compile(r'\\b(HGNC:\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # Human Protein Atlas (HPA) identifiers\n",
    "    'hpa_id': re.compile(r'\\b((?:HPA|CAB)\\d{6})\\b', re.IGNORECASE),\n",
    "\n",
    "    # IGSR (International Genome Sample Resource) / 1000 Genomes identifiers\n",
    "    'igsr_id': re.compile(r'\\b(HG0[0-4]\\d{3}|(?:NA|GM)[0-2]\\d{4})\\b'),\n",
    "    \n",
    "    # IntAct molecular interaction database ID\n",
    "    'intact_id': re.compile(r'\\b(EBI-\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # MINT (Molecular INTeraction database) ID\n",
    "    'mint_id': re.compile(r'\\b((?:MINT|IM)-\\d+)\\b', re.IGNORECASE),\n",
    "    \n",
    "    # InterPro ID for protein families, domains and sites\n",
    "    'interpro_id': re.compile(r'\\b(IPR\\d{6})\\b', re.IGNORECASE),\n",
    "\n",
    "    # MetaboLights study accession ID\n",
    "    'metabolights_id': re.compile(r'\\b(MTBLS\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # Protein Data Bank (PDB) ID\n",
    "    'pdb_id': re.compile(r'\\b(\\d[a-zA-Z0-9]{3})\\b'),\n",
    "\n",
    "    # Pfam database accession ID\n",
    "    'pfam_id': re.compile(r'\\b(PF(?:AM)?\\d{5})\\b', re.IGNORECASE),\n",
    "\n",
    "    # PRIDE/ProteomeXchange dataset identifier\n",
    "    'pride_id': re.compile(r'\\b(R?PXD\\d{6})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Reactome pathway identifier\n",
    "    'reactome_id': re.compile(r'\\b(R-HSA-\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # Rfam accession ID\n",
    "    'rfam_id': re.compile(r'\\b(RF\\d{5})\\b'),\n",
    "\n",
    "    # Rhea reaction identifier\n",
    "    'rhea_id': re.compile(r'\\b(RHEA:[1-9]\\d*)\\b', re.IGNORECASE),\n",
    "\n",
    "    # RNAcentral sequence identifier\n",
    "    'rnacentral_id': re.compile(r'\\b(URS[0-9A-Z]+_\\d+)\\b'),\n",
    "    \n",
    "    # UniProt accession numbers\n",
    "    # 'uniprot_id': re.compile(r'\\b((?:[A-NR-Z]\\d[A-Z][A-Z0-9]{2}\\d)|(?:[OPQ]\\d[A-Z0-9]{3}\\d))(?:-\\d+)?\\b', re.IGNORECASE),\n",
    "    'uniprot_id': re.compile(r'\\b((?:[A-NR-Z]\\d[A-Z][A-Z0-9]{2,}\\d)|(?:[OPQ]\\d[A-Z0-9]{3,}\\d))(?:-\\d+)?\\b', re.IGNORECASE),\n",
    "    # UniParc (UniProt Archive) identifier\n",
    "    'uniparc_id': re.compile(r'\\b(UPI[A-F0-9]{10})\\b', re.IGNORECASE),\n",
    "    \n",
    "    # EBiSC (European Bank for induced pluripotent Stem Cells) ID\n",
    "    'ebisc_id': re.compile(r'\\b([A-Z]{2,}i\\d{3}-[A-Z])\\b'),\n",
    "\n",
    "    # HipSci (Human Induced Pluripotent Stem Cells Initiative) ID\n",
    "    'hipsci_id': re.compile(r'\\b(HPSI\\d{4}(?:i|pf)-[a-z]+_\\d+)\\b'),\n",
    "\n",
    "    # RefSeq sequence identifier\n",
    "    'refseq_id': re.compile(r'\\b((?:AC|AP|NC|NG|NM|NP|NR|NT|NW|NZ|XM|XP|XR|YP|ZP|NS)_(?:[A-Z]{4})*\\d{6,9}(?:\\.\\d+)?)\\b', re.IGNORECASE),\n",
    "\n",
    "    # dbSNP (Single Nucleotide Polymorphism database) ID\n",
    "    'refsnp_id': re.compile(r'\\b([rs]s\\d{1,9})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Digital Object Identifier (DOI)\n",
    "    # 'doi': re.compile(r'\\b(10\\.\\d{4,}/[^ ()\\\"<>]+)\\b'),\n",
    "\n",
    "    # BioProject accession ID\n",
    "    'bioproject_id': re.compile(r'\\b(PRJ[DEN][A-Z]\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # GenBank Assembly Accession (GCA)\n",
    "    # 'gca_id': re.compile(r'\\b(GCA_\\d{9}(?:\\.\\d+)?)\\b'),\n",
    "\n",
    "    # TreeFam (Tree Families database) ID\n",
    "    'treefam_id': re.compile(r'\\b(TF\\d{6})\\b', re.IGNORECASE),\n",
    "    \n",
    "    # EudraCT (European Union Drug Regulating Authorities Clinical Trials Database) number\n",
    "    'eudract_id': re.compile(r'(\\d{4}-\\d{6}-\\d{2})'),\n",
    "\n",
    "    # ClinicalTrials.gov ID\n",
    "    'nct_id': re.compile(r'\\b(NCT0\\d{7})\\b', re.IGNORECASE),\n",
    "\n",
    "    # dbGaP (database of Genotypes and Phenotypes) study accession\n",
    "    'dbgap_id': re.compile(r'\\b(phs\\d{6})\\b', re.IGNORECASE),\n",
    "\n",
    "    # GEO (Gene Expression Omnibus) accession ID\n",
    "    'geo_id': re.compile(r'\\b(G(?:PL|SM|SE|DS)\\d{2,})\\b', re.IGNORECASE),\n",
    "\n",
    "    # Orphadata/Orphanet identifier for rare diseases\n",
    "    # 'orphanet_id': re.compile(r'\\b(ORPHA[: ]\\d+)\\b', re.IGNORECASE),\n",
    "\n",
    "    # GISAID (Global Initiative on Sharing All Influenza Data) identifiers\n",
    "    'gisaid_id': re.compile(r'(EPI\\d{6,}|EPI_ISL_\\d{5,})', re.IGNORECASE),\n",
    "    \"biostudies\": re.compile(r'\\b(S-[A-Z]{3,5}\\d+)\\b'),\n",
    "}\n",
    "COMPILED_PATTERNS = {\n",
    "    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],    \n",
    "    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "    ],\n",
    "}\n",
    "BIBLIOGRAPHY_PATTERNS = [\n",
    "    # References patterns\n",
    "    r'^REFERENCES?$',\n",
    "    r'^\\d+\\.?\\s+REFERENCES?$',\n",
    "    r'^\\d+\\.?\\s+References?$',\n",
    "    r'^References?:?$',\n",
    "    r'^References?\\s+Cited.{0,10}$',\n",
    "    # Bibliography patterns  \n",
    "    r'^BIBLIOGRAPHY$',\n",
    "    r'^\\d+\\.?\\s+BIBLIOGRAPHY$',\n",
    "    r'^\\d+\\.?\\s+Bibliography$',\n",
    "    r'^Bibliography:?$',\n",
    "\n",
    "    # Other common patterns\n",
    "    r'^Literature\\s+Cited$',\n",
    "    r'^Works\\s+Cited.{0,10}$',\n",
    "    r'^ACKNOWLEDGMENTS?$',\n",
    "    r'^Acknowledgments?$',\n",
    "    r'^FUNDING$',\n",
    "    r'^CONFLICTS?\\s+OF\\s+INTEREST$',\n",
    "    r'^NOTES$',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.672131Z",
     "iopub.status.busy": "2025-08-29T07:53:44.671772Z",
     "iopub.status.idle": "2025-08-29T07:53:44.681952Z",
     "shell.execute_reply": "2025-08-29T07:53:44.68097Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.6721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_all_accession_ids_efficient(base_path='/kaggle/input/accession-ids-dataset'):\n",
    "    \"\"\"\n",
    "    高效版本：逐行读取，避免内存溢出。\n",
    "    同时收集每个 ID 对应的 PMCID 和 EXTID 列表。\n",
    "    \"\"\"\n",
    "    all_ids = set()\n",
    "    # 新增字典用于存储 ID 对应的 PMCID 和 EXTID\n",
    "    id_details = {} # key: ID, value: {'pmc_ids': set(), 'ext_ids': set()}\n",
    "    \n",
    "    csv_files = list(Path(base_path).glob('*.csv'))\n",
    "    print(f\"找到 {len(csv_files)} 个CSV文件\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # 假设列名是文件名（不含扩展名）\n",
    "        column_name = csv_file.stem \n",
    "        \n",
    "        try:\n",
    "            # 分块读取大文件\n",
    "            chunk_size = 10000\n",
    "            for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "                if column_name in chunk.columns:\n",
    "                    # 处理当前块\n",
    "                    \n",
    "                    # 1. 提取主ID（原逻辑）\n",
    "                    ids_series = chunk[column_name].dropna().astype(str)\n",
    "                    ids_list = ids_series.tolist()\n",
    "                    all_ids.update(ids_list)\n",
    "                    \n",
    "                    # 2. 提取 PMCID 和 EXTID (如果列存在)\n",
    "                    pmc_col = 'PMCID'\n",
    "                    ext_col = 'EXTID'\n",
    "                    \n",
    "                    # 确保列存在再处理\n",
    "                    if pmc_col in chunk.columns and ext_col in chunk.columns:\n",
    "                        # 遍历当前块的行，填充 id_details 字典\n",
    "                        for _, row in chunk.iterrows():\n",
    "                            main_id = row.get(column_name)\n",
    "                            if pd.isna(main_id):\n",
    "                                continue # 跳过主ID为空的行\n",
    "                            main_id = str(main_id)\n",
    "                            \n",
    "                            # 初始化字典条目\n",
    "                            if main_id not in id_details:\n",
    "                                id_details[main_id] = {'pmc_ids': set(), 'ext_ids': set()}\n",
    "                            \n",
    "                            # 添加 PMCID (如果非空)\n",
    "                            pmc_id = row.get(pmc_col)\n",
    "                            if not pd.isna(pmc_id):\n",
    "                                id_details[main_id]['pmc_ids'].add(str(pmc_id))\n",
    "                            \n",
    "                            # 添加 EXT_ID (如果非空)\n",
    "                            ext_id = row.get(ext_col)\n",
    "                            if not pd.isna(ext_id):\n",
    "                                id_details[main_id]['ext_ids'].add(str(ext_id))\n",
    "                    # 如果列不存在，可以打印警告或忽略\n",
    "                    # else:\n",
    "                    #     print(f\"  警告: 文件 {csv_file.name} 缺少 '{pmc_col}' 或 '{ext_col}' 列\")\n",
    "                    \n",
    "            print(f\"✓ {column_name}: 处理完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {csv_file.name}: 读取错误 - {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n总共收集到 {len(all_ids)} 个唯一的accession IDs\")\n",
    "    # 可选：打印 id_details 的大小作为调试信息\n",
    "    # print(f\"收集到 {len(id_details)} 个 ID 的详细信息\")\n",
    "    \n",
    "    # 返回集合和字典\n",
    "    return all_ids, id_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:35:16.182939Z",
     "iopub.status.busy": "2025-08-29T08:35:16.181747Z",
     "iopub.status.idle": "2025-08-29T08:35:16.203167Z",
     "shell.execute_reply": "2025-08-29T08:35:16.202305Z",
     "shell.execute_reply.started": "2025-08-29T08:35:16.182905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 我们将部分具有代表性的上下文规则提取出来，并编译正则表达式以提高效率\n",
    "# 注意：(?i) 标志在XML中，我们在 re.compile 中使用 re.IGNORECASE 来实现\n",
    "# import re\n",
    "bad_ids_acc ={\"cath_domain\", \"pdb_id\", \"refsnp_id\", \"ena_accession\", \"uniprot_id\", \"cath_id\", 'brenda_ec_number'}\n",
    "CONTEXT_VALIDATION_RULES = {\n",
    "    'arrayexpress': {\n",
    "        'context_regex': re.compile(r'(arrayexpress|atlas|gxa|accession|experiment)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'alphafold': {\n",
    "        'context_regex': re.compile(r'(alphafold|alphafold database|alphafold db|structures|predicted structure|predicted protein structure|protein|identifier|accession)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'brenda': {\n",
    "        'context_regex': re.compile(r'(BRENDA enzyme|BRENDA enzyme database|BRaunschweig ENzyme DAtabase|enzyme database|enzyme|lysosomes|lysosomal|BRENDA:|BRENDA: |BRENDA:EC|BRENDA:EC|BRENDA tissue ontology|BTO|ontology)', re.IGNORECASE),\n",
    "        'window_size': 2000\n",
    "    },\n",
    "    'bia': {\n",
    "        'context_regex': re.compile(r'(bia|bioimage archive database|bioimage archive|database|identifier|accession)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'biomodels': {\n",
    "        'context_regex': re.compile(r'(biomodels|accession|model|identifier)', re.IGNORECASE),\n",
    "        'window_size': 3000\n",
    "    },\n",
    "    'biosample': {\n",
    "        'context_regex': re.compile(r'(biosample|accession|model)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'cath': {\n",
    "        'context_regex': re.compile(r'(cath|cath-Gene3D|cath Gene3D|c\\.a\\.t\\.h|domain|families|cathnode|pdb|superfamily)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'cellosaurus': {\n",
    "        'context_regex': re.compile(r'(cells|cellosaurus|cellosaurus database|Cell lines|Cell Bank|cell lines|cell bank|accession number|RRID:)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'chebi': {\n",
    "        'context_regex': re.compile(r'(chebi|compound)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'chembl': {\n",
    "        'context_regex': re.compile(r'(chembl|compound)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'complexportal': {\n",
    "        'context_regex': re.compile(r'(protein|complex)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'metagenomics': {\n",
    "        'context_regex': re.compile(r'(samples|ebi metagenomics|metagenomics|database)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'ega': { # Combined from ega.study, ega.dataset, ega.dac\n",
    "        'context_regex': re.compile(r'(ega|accession|archive|studies|study|dataset|datasets|data set|data sets|validation sets|validation set|set|sets|data|dac|European Genome-phenome Archive|European Genome phenome Archive)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'emdb': {\n",
    "        'context_regex': re.compile(r'(emdb|accession|code)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'ena': { # Combined from multiple gen/ena rules\n",
    "        'context_regex': re.compile(r'(genbank|\\bgen\\b|\\bena\\b|ddbj|embl|european nucleotide archive|accession|nucleotide|archive|asssembled|annotated|sequence|sequences|protein coding|protein|trace|traces|study|studies|sample|samples|experiment|experiments|run|runs|analysis|analyses|submission|submissions)', re.IGNORECASE),\n",
    "        'window_size': 2000\n",
    "    },\n",
    "    'ensembl': {\n",
    "        'context_regex': re.compile(r'(ensembl|accession|transcript|sequence)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'go': {\n",
    "        'context_regex': re.compile(r'(go|gene ontology)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'hgnc': {\n",
    "        'context_regex': re.compile(r'(HUGO Gene Nomenclature Committee|hugo|gene|nomenclature|committee|database)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'igsr': {\n",
    "        'context_regex': re.compile(r'(\\bcell\\b|sample|iPSC|iPSCs|iPS|fibroblast|fibroblasts|QTL|eQTL|pluripotent|induced|\\bdonor\\b|\\bstem\\b|EBiSC|1000 Genomes|Coriell|\\bLCL\\b|lymphoblastoid)', re.IGNORECASE),\n",
    "        'window_size': 3000\n",
    "    },\n",
    "    'intact': {\n",
    "        'context_regex': re.compile(r'(intact|IntAct|inTact|Intact|interaction|interactions|protein)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'mint': {\n",
    "        'context_regex': re.compile(r'(MINT|molecular interaction database|interactions|interaction)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'interpro': {\n",
    "        'context_regex': re.compile(r'(interpro|domain|family|motif|accession)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'metabolights': {\n",
    "        'context_regex': re.compile(r'(metabolights|accession|repository)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'pdb': {\n",
    "        'context_regex': re.compile(r'(pdb|(?:protein\\s+data\\s*bank)|accession|structure|domain)', re.IGNORECASE),\n",
    "        'window_size': 1000\n",
    "    },\n",
    "    'pfam': {\n",
    "        'context_regex': re.compile(r'(pfam|domain|family|accession|motif)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'pxd': {\n",
    "        'context_regex': re.compile(r'(pxd|proteomexchange|pride|dataset|accession|repository)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'reactome': {\n",
    "        'context_regex': re.compile(r'(biological|regulatory|pathway|pathways|database)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'rhea': {\n",
    "        'context_regex': re.compile(r'(reactions|database|rhea database|accession)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'uniprot': {\n",
    "        'context_regex': re.compile(r'(swiss-prot|sprot|uniprot|swiss prot|accession(s)?|Locus|GenBank|genome|sequence(s)?|protein|trembl|uniparc|uniprotkb|Acc\\.No|Acc\\. No)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'uniparc': {\n",
    "        'context_regex': re.compile(r'(uniprot|accession(s)?|Locus|sequence(s)?|protein|uniparc|Acc\\.No|Acc\\. No)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'ebisc': {\n",
    "        'context_regex': re.compile(r'(\\bcell\\b|sample|iPSC|iPSCs|iPS|fibroblast|fibroblasts|QTL|eQTL|pluripotent|induced|\\bdonor\\b|\\bstem\\b|EBiSC|1000 Genomes|Coriell|\\bLCL\\b|lymphoblastoid)'),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'hipsci': {\n",
    "        'context_regex': re.compile(r'(\\bcell\\b|sample|iPSC|iPSCs|iPS|fibroblast|fibroblasts|QTL|eQTL|pluripotent|induced|\\bdonor\\b|\\bstem\\b|EBiSC|1000 Genomes|Coriell|\\bLCL\\b|lymphoblastoid)'),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'refseq': {\n",
    "        'context_regex': re.compile(r'(refseq|genbank|accession|sequence)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'refsnp': {\n",
    "        'context_regex': re.compile(r'(allele|model|multivariate|polymorphism|locus|loci|haplotype|genotype|variant|chromosome|SNPs|snp|snp(s)*)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'doi': {\n",
    "        'context_regex': re.compile(r'(doi|repository)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'bioproject': {\n",
    "        'context_regex': re.compile(r'(bioproject|accession|archive)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'treefam': {\n",
    "        'context_regex': re.compile(r'(treefam|tree|family|accession|dendrogram)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'eudract': {\n",
    "        'context_regex': re.compile(r'(eudract|trial|agency|register|clinical)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'nct': {\n",
    "        'context_regex': re.compile(r'(trial)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'dbgap': {\n",
    "        'context_regex': re.compile(r'(database of genotypes and phenotypes|dbgap|accession|archives|studies|interaction)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'geo': {\n",
    "        'context_regex': re.compile(r'(gene expression omnibus|genome|geo|accession|functional genomics|data repository|data submissions)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'orphadata': {\n",
    "        'context_regex': re.compile(r'(database|rare disease|disease|data|nomenclature|syndrome|id|number|name|orphanet|orphadata|orpha)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    'gisaid': {\n",
    "        'context_regex': re.compile(r'(gisaid|global initiative on sharing all influenza data|segment|segments|identifier|flu|epi|epiflu|database|sequence|sequences|isolate|isolates|accession|virus|viruses|strain|strains)', re.IGNORECASE),\n",
    "        'window_size': 5000\n",
    "    },\n",
    "    \n",
    "    # # --- Databases without Context Validation ---\n",
    "    # 'efo': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "    # 'empiar': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "    # 'hpa': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "    # 'rfam': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "    # 'rnacentral': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "    # 'gca': {\n",
    "    #     'context_regex': None,\n",
    "    #     'window_size': 0\n",
    "    # },\n",
    "}\n",
    "SPECIAL_MAPPINGS = {\n",
    "    'sra_id': 'ena',\n",
    "    'ena_trace': 'ena',\n",
    "    'pride_id': 'pxd',\n",
    "}\n",
    "def is_context_valid(text: str, context_regex: re.Pattern) -> bool:\n",
    "    \"\"\"\n",
    "    检查一个正则表达式匹配项（match object）的上下文是否有效。\n",
    "\n",
    "    Args:\n",
    "        match: re.finditer 返回的 match 对象。\n",
    "        text: 供搜索的完整原始文本。\n",
    "        context_regex: 用于验证上下文的已编译的正则表达式。\n",
    "        window_size: 在匹配项前后搜索上下文关键字的字符数。\n",
    "\n",
    "    Returns:\n",
    "        如果上下文有效，则返回 True，否则返回 False。\n",
    "    \"\"\"\n",
    "    # 获取匹配项在文本中的起始和结束位置\n",
    "    match_start, match_end = match.span()\n",
    "\n",
    "    # 根据 window_size 计算上下文搜索窗口的边界\n",
    "    # 确保窗口边界不会超出文本的实际长度\n",
    "    window_start = max(0, match_start - window_size)\n",
    "    window_end = min(len(text), match_end + window_size)\n",
    "\n",
    "    # 提取上下文窗口的文本\n",
    "    context_window_text = text[window_start:window_end]\n",
    "\n",
    "    # 在上下文窗口中搜索关键字\n",
    "    if context_regex.search(text):\n",
    "        return True  # 找到了上下文关键字，验证通过\n",
    "    else:\n",
    "        return False # 未找到上下文关键字，验证失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.727205Z",
     "iopub.status.busy": "2025-08-29T07:53:44.726853Z",
     "iopub.status.idle": "2025-08-29T07:53:44.748054Z",
     "shell.execute_reply": "2025-08-29T07:53:44.747311Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.727179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASSIFICATION = {\n",
    "    \"primary\": re.compile(r'(deposited|accessible|submitted|measured|collected|observed|detected|analyzed|generated|produced|obtained|recorded|sequenced|amplified|isolated|vailability|vailable|novel|hosted|raw(?:\\s+data)?)|supplementary'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.749342Z",
     "iopub.status.busy": "2025-08-29T07:53:44.74902Z",
     "iopub.status.idle": "2025-08-29T07:53:44.774686Z",
     "shell.execute_reply": "2025-08-29T07:53:44.773766Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.749315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_and_cluster_keywords(body_text, rules, window_size=100, cluster_gap=50):\n",
    "    \"\"\"\n",
    "    在文本中查找关键词，将邻近的匹配项聚类，并为每个聚类提取唯一的上下文窗口。\n",
    "    \"\"\"\n",
    "    all_matches = []\n",
    "    for prefix, keyword_reg in rules.items():\n",
    "        for match in keyword_reg.finditer(body_text):\n",
    "            all_matches.append({'prefix': prefix, 'match': match})\n",
    "\n",
    "    if not all_matches:\n",
    "        return {}\n",
    "\n",
    "    all_matches.sort(key=lambda x: x['match'].start())\n",
    "\n",
    "    clusters = []\n",
    "    if all_matches:\n",
    "        current_cluster = [all_matches[0]]\n",
    "        clusters.append(current_cluster)\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_item = all_matches[i]\n",
    "            last_item_in_cluster = current_cluster[-1]\n",
    "            gap = current_item['match'].start() - last_item_in_cluster['match'].end()\n",
    "            if gap < cluster_gap:\n",
    "                current_cluster.append(current_item)\n",
    "            else:\n",
    "                current_cluster = [current_item]\n",
    "                clusters.append(current_cluster)\n",
    "\n",
    "    final_results = {}\n",
    "    for cluster in clusters:\n",
    "        main_prefix = cluster[0]['prefix']\n",
    "        cluster_start = cluster[0]['match'].start()\n",
    "        cluster_end = cluster[-1]['match'].end()\n",
    "        window_start = max(0, cluster_start - window_size)\n",
    "        window_end = min(len(body_text), cluster_end + window_size)\n",
    "        context_window_text = body_text[window_start:window_end].strip()\n",
    "        \n",
    "        context_list = final_results.setdefault(main_prefix, [])\n",
    "        if context_window_text not in context_list:\n",
    "            context_list.append(context_window_text)\n",
    "            \n",
    "    return final_results\n",
    "DOI_KEYWORD_RULES = {\n",
    "    # Key: DOI Prefix (string)\n",
    "    # Value: Compiled regex of associated keywords\n",
    "    \n",
    "    \"10.15468\": re.compile(r\"(GBIF|Global Biodiversity Information Facility)\"),\n",
    "    \"10.5061\": re.compile(r\"(Dryad|Dryad Digital Repository)\"),\n",
    "    \"10.5281\": re.compile(r\"(Zenodo|CERN)\"),\n",
    "    \"10.6073\": re.compile(r\"(PASTA|EDI|Environmental Data Initiative)\"),\n",
    "    \"10.17605\": re.compile(r\"(OSF|Open Science Framework)\"),\n",
    "    \"10.5256\": re.compile(r\"(F1000Research)\"),\n",
    "    \"10.6084\": re.compile(r\"(Figshare)\"),\n",
    "    \"10.1594\": re.compile(r\"(Pangaea|Data Publisher for Earth & Environmental Science)\"),\n",
    "    \"10.7910\": re.compile(r\"(Dataverse|Harvard Dataverse)\"),\n",
    "    \"10.3886\": re.compile(r\"(ICPSR|Inter-university Consortium for Political and Social Research)\")\n",
    "}\n",
    "import re\n",
    "\n",
    "# (最终版本) 完整覆盖所有参考规则，并遵循“最长优先”原则的字典\n",
    "ACCESSION_ID_KEYWORD_RULES = {\n",
    "    # --- 序列、基因组与变异数据库 ---\n",
    "    \"ena\": re.compile(r\"(European\\sNucleotide\\sArchive|GenBank|DDBJ|ENA|EMBL)\", re.IGNORECASE),\n",
    "    \"refseq\": re.compile(r\"(Reference\\sSequence|RefSeq)\", re.IGNORECASE),\n",
    "    \"ensembl\": re.compile(r\"(Ensembl)\", re.IGNORECASE),\n",
    "    \"refsnp\": re.compile(r\"(dbSNP|rsID|SNP)\", re.IGNORECASE),\n",
    "    \"ega\": re.compile(r\"(European\\sGenome-phenome\\sArchive|EGA)\", re.IGNORECASE),\n",
    "    \"dbgap\": re.compile(r\"(database\\sof\\sgenotypes\\sand\\sphenotypes|dbgap)\", re.IGNORECASE),\n",
    "    \"gisaid\": re.compile(r\"(gisaid|epiflu|EPI_ISL)\", re.IGNORECASE),\n",
    "    \"metagenomics\": re.compile(r\"(ebi\\smetagenomics)\", re.IGNORECASE),\n",
    "    \"hgnc\": re.compile(r\"(HUGO\\sGene\\sNomenclature\\sCommittee|hgnc)\", re.IGNORECASE),\n",
    "    \"treefam\": re.compile(r\"(treefam)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 蛋白质与结构数据库 ---\n",
    "    \"uniprot\": re.compile(r\"(swiss-prot|swiss\\sprot|UniProtKB|TrEMBL|UniProt|Sprot)\", re.IGNORECASE),\n",
    "    \"uniparc\": re.compile(r\"(uniparc)\", re.IGNORECASE),\n",
    "    \"pdb\": re.compile(r\"(Protein\\sData\\sBank|PDB)\", re.IGNORECASE),\n",
    "    \"alphafold\": re.compile(r\"(AlphaFold\\sDB|AlphaFold)\", re.IGNORECASE),\n",
    "    \"pfam\": re.compile(r\"(Pfam)\", re.IGNORECASE),\n",
    "    \"interpro\": re.compile(r\"(InterPro)\", re.IGNORECASE),\n",
    "    \"cath\": re.compile(r\"(cath-Gene3D|cath)\", re.IGNORECASE),\n",
    "    \"emdb\": re.compile(r\"(emdb)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 功能基因组学与表达数据库 ---\n",
    "    \"geo\": re.compile(r\"(Gene\\sExpression\\sOmnibus|GEO)\", re.IGNORECASE),\n",
    "    \"arrayexpress\": re.compile(r\"(ArrayExpress|gxa|atlas)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 化学、代谢物与反应数据库 ---\n",
    "    \"chebi\": re.compile(r\"(ChEBI)\", re.IGNORECASE),\n",
    "    \"chembl\": re.compile(r\"(ChEMBL)\", re.IGNORECASE),\n",
    "    \"brenda\": re.compile(r\"(BRaunschweig\\sENzyme\\sDAtabase|BRENDA\\senzyme|BRENDA|BTO)\", re.IGNORECASE),\n",
    "    \"rhea\": re.compile(r\"(rhea)\", re.IGNORECASE),\n",
    "    \"metabolights\": re.compile(r\"(metabolights)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 系统生物学与通路数据库 ---\n",
    "    \"biomodels\": re.compile(r\"(BioModels)\", re.IGNORECASE),\n",
    "    \"reactome\": re.compile(r\"(Reactome)\", re.IGNORECASE),\n",
    "    \"intact\": re.compile(r\"(IntAct)\", re.IGNORECASE),\n",
    "    \"mint\": re.compile(r\"(molecular\\sinteraction\\sdatabase|MINT)\", re.IGNORECASE),\n",
    "    \"complexportal\": re.compile(r\"(ComplexPortal)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 样本、细胞系与项目数据库 ---\n",
    "    \"bioproject\": re.compile(r\"(BioProject|NCBI\\sBioProject)\", re.IGNORECASE),\n",
    "    \"biosample\": re.compile(r\"(BioSample|NCBI\\sBioSample)\", re.IGNORECASE),\n",
    "    \"cellosaurus\": re.compile(r\"(Cellosaurus|CVCL)\", re.IGNORECASE),\n",
    "    \"igsr\": re.compile(r\"(1000\\sGenomes|Coriell|igsr)\", re.IGNORECASE),\n",
    "    \"ebisc\": re.compile(r\"(EBiSC)\", re.IGNORECASE),\n",
    "    \"hipsci\": re.compile(r\"(HipSci)\", re.IGNORECASE),\n",
    "    \"pxd\": re.compile(r\"(ProteomeXchange|PRIDE|PXD)\", re.IGNORECASE),\n",
    "    \"bia\": re.compile(r\"(bioimage\\sarchive|bia)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 本体论与疾病数据库 ---\n",
    "    \"go\": re.compile(r\"(Gene\\sOntology|GO)\", re.IGNORECASE),\n",
    "    \"orphadata\": re.compile(r\"(Orphanet|Orphadata|ORPHA)\", re.IGNORECASE),\n",
    "\n",
    "    # --- 临床试验数据库 ---\n",
    "    \"nct\": re.compile(r\"(ClinicalTrials.gov|NCT)\", re.IGNORECASE),\n",
    "    \"eudract\": re.compile(r\"(EudraCT)\", re.IGNORECASE),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.776183Z",
     "iopub.status.busy": "2025-08-29T07:53:44.775833Z",
     "iopub.status.idle": "2025-08-29T07:53:44.806085Z",
     "shell.execute_reply": "2025-08-29T07:53:44.805196Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.776153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_broken_links(text: str) -> str:\n",
    "    patterns = [\n",
    "        (r'https?://\\s*([a-zA-Z0-9\\.\\-]+)', r'https://\\1'),\n",
    "        (r'(doi\\.)\\s+(org/10\\.)', r'\\1\\2'),\n",
    "        (r'doi\\.org/\\s+10\\.', r'doi.org/10.'),\n",
    "        (r'doi\\.org/10\\.\\s*(\\d+)', r'doi.org/10.\\1'),\n",
    "        (r'doi\\.org/10\\.(\\d{4,5})/\\s+([a-zA-Z0-9\\.\\-_]+)', r'doi.org/10.\\1/\\2'),\n",
    "        # (r'(https?://[a-zA-Z0-9\\.\\-/]+/)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "        # (r'(https?://[a-zA-Z0-9\\.\\-/]+-)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "        (r'(10\\.\\d{4,5}/[a-zA-Z0-9\\.\\-/]+\\.)\\s+([a-z0-9\\.\\-_/]{4,20},?\\s+|(?!(?:DATAS?|RESULTS?|DISCUSSIONS?|REFERENCES?|CONFLICT|ORCID|ACKNOWLEDGMENTS?|References?)\\b)[A-Z0-9\\.\\-_/]{4,20}\\s+)', r'\\1\\2'),\n",
    "        # (r'(https?://[^\\s]*[?&])\\s+([a-zA-Z0-9=&\\.\\-_]+)', r'\\1\\2'),\n",
    "        (r'\\n+(https?://doi\\.org/[^\\s]*)', r'\\1'),\n",
    "        (r'(\\(.{0,20}10\\.\\d{4,}/.{1,30})\\s+([A-Za-z0-9]{1,10}\\))', r'\\1\\2'),\n",
    "        (r'(10\\.\\d{4,5}/[a-zA-Z0-9\\.\\-/]+-)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "        (r'(doi\\.org/10\\.\\d{4,5}/[a-zA-Z0-9\\.\\-/]+-)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "        (r'(https://doi\\.org/10\\.\\d{4,5}/[a-zA-Z0-9\\.\\-/]+-)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "        (r'(10\\.\\d{4,5}/[a-zA-Z0-9\\.\\-/]+-)\\s+([a-zA-Z0-9\\.\\-_/]+)', r'\\1\\2'),\n",
    "    ]\n",
    "    for pattern, replacement in patterns:\n",
    "        text = re.sub(pattern, replacement, text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text\n",
    "def add_period_to_heading_lines(text):\n",
    "    \"\"\"\n",
    "    在所有匹配BIBLIOGRAPHY_PATTERNS的标题行末尾添加句点\n",
    "    条件：行必须完全匹配预定义模式且不以标点结尾\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].rstrip()  # 只移除右侧空白\n",
    "        \n",
    "        # 检查是否匹配任何参考文献/标题模式\n",
    "        if any(re.fullmatch(pattern, line, re.IGNORECASE) for pattern in BIBLIOGRAPHY_PATTERNS):\n",
    "            # 检查行尾是否已有标点\n",
    "            # print(\"有目标\",line)\n",
    "            if not re.search(r'[.]$', line):\n",
    "                lines[i] = '\\n\\n' + line + '\\n\\n'\n",
    "                # print(f\"添加逗号: {line}\")\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    对单个字符串进行标准化处理：\n",
    "    1. Unicode 标准化（NFKC）\n",
    "    2. 移除非 ASCII 字符\n",
    "    3. 将 Zenodo URL 转换为 DOI 格式\n",
    "    \"\"\"\n",
    "    # 1. Unicode 标准化\n",
    "    normalized = unicodedata.normalize(\"NFKC\", text)\n",
    "    # 2. 移除非 ASCII 字符\n",
    "    ascii_only = re.sub(r\"[^\\x00-\\x7F]\", \"\", normalized)\n",
    "    # 3. Zenodo URL 转 DOI\n",
    "    zenodo_doi = re.sub(\n",
    "        r\"https?://zenodo\\.org/record/(\\d+)\",\n",
    "        r\" 10.5281/zenodo.\\1 \",\n",
    "        ascii_only\n",
    "    )\n",
    "    return zenodo_doi.strip()  # 移除多余空格\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n",
    "                        return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None:\n",
    "                return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "def is_balanced(text: str) -> bool:\n",
    "    \"\"\"检查字符串中的括号是否平衡\"\"\"\n",
    "    paren_stack = 0  # 圆括号计数器\n",
    "    bracket_stack = 0  # 方括号计数器\n",
    "    \n",
    "    for char in text:\n",
    "        if char == '(':\n",
    "            paren_stack += 1\n",
    "        elif char == ')':\n",
    "            paren_stack -= 1\n",
    "            if paren_stack < 0:  # 出现多余的闭括号\n",
    "                return False\n",
    "        elif char == '[':\n",
    "            bracket_stack += 1\n",
    "        elif char == ']':\n",
    "            bracket_stack -= 1\n",
    "            if bracket_stack < 0:\n",
    "                return False\n",
    "    \n",
    "    return paren_stack == 0 and bracket_stack == 0  # 所有括号必须完全闭合\n",
    "def remove_references_section_v2(text):\n",
    "    lines = text.split('\\n')\n",
    "    cut_index = -1\n",
    "    \n",
    "    # Look backwards from end of document\n",
    "    for i in range(len(lines) - 1, max(0, int(len(lines) * 0.3)), -1):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if any(re.match(pattern, line, re.IGNORECASE) for pattern in BIBLIOGRAPHY_PATTERNS):\n",
    "            # Double-check: look at following lines for citation patterns\n",
    "            following_lines = lines[i+1:i+5]  # Check more lines\n",
    "            has_citations = False\n",
    "            \n",
    "            for follow_line in following_lines:\n",
    "                if follow_line.strip():\n",
    "                    # Check for obvious citation patterns\n",
    "                    if (re.search(r'\\(\\d{4}\\)', follow_line) or\n",
    "                        re.search(r'\\d{4}\\.', follow_line) or\n",
    "                        'doi:' in follow_line.lower() or\n",
    "                        ' et al' in follow_line.lower() or\n",
    "                        re.search(r'^\\[\\d+\\]', follow_line.strip()) or  # [1], [2], etc.\n",
    "                        re.search(r'^\\d+\\.', follow_line.strip())):     # 1., 2., etc.\n",
    "                        has_citations = True\n",
    "                        break\n",
    "            \n",
    "            # Only cut if we found citation-like content\n",
    "            if has_citations or i >= len(lines) - 3:  # Or very near end\n",
    "                cut_index = i\n",
    "                break\n",
    "    \n",
    "    if cut_index != -1:\n",
    "        return '\\n'.join(lines[:cut_index]).strip(),'\\n'.join(lines[cut_index:]).strip()\n",
    "    \n",
    "    return text.strip(),\"\"\n",
    "def remove_references_section_v3(text):\n",
    "    lines = text.split('\\n')\n",
    "    cut_index = -1\n",
    "\n",
    "    # 从文本的后70%部分开始，从后向前搜索\n",
    "    # 这是一个合理的启发式搜索，避免在长文档的开头部分进行不必要的工作\n",
    "    start_search_index = int(len(lines) * 0.3)\n",
    "    for i in range(len(lines) - 1, start_search_index, -1):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # 检查当前行是否与某个标题模式完全匹配\n",
    "        if any(re.fullmatch(pattern, line, re.IGNORECASE) for pattern in BIBLIOGRAPHY_PATTERNS):\n",
    "            \n",
    "            # --- 开始进行更严格的“双重验证” ---\n",
    "\n",
    "            # 验证 1: 检查该行是否符合典型的“标题格式”\n",
    "            is_heading_format = (\n",
    "                len(line) < 40 and  # 标题长度通常不会太长\n",
    "                (line.isupper() or line.istitle()) # 典型格式：全大写或首字母大写\n",
    "            )\n",
    "\n",
    "            # 验证 2: 统计后续几行中的“引文特征”数量\n",
    "            citation_features_count = 0\n",
    "            following_lines_to_check = lines[i+1 : i+10]  # 向后看7行进行验证\n",
    "            \n",
    "            for follow_line in following_lines_to_check:\n",
    "                if (re.search(r'\\((19|20)\\d{2}\\)', follow_line) or  # (Author, 2020)\n",
    "                    re.search(r'\\b(19|20)\\d{2}[a-z]?\\b', follow_line) or  # 2020 or 2020a\n",
    "                    'doi:' in follow_line.lower() or\n",
    "                    'et al' in follow_line.lower() or\n",
    "                    re.search(r'^\\[\\d+\\]', follow_line.strip()) or  # [1]\n",
    "                    re.search(r'^\\d+\\.\\s', follow_line.strip()) or\n",
    "                    re.search(r'[A-Z]\\.', follow_line.strip())):   # 1. Author\n",
    "                        citation_features_count += 1\n",
    "            \n",
    "            # --- 决策逻辑 ---\n",
    "            # 如果它看起来像一个标题，或者其后紧跟着高密度的引文内容，\n",
    "            # 我们就认为找到了切分点。\n",
    "            # (阈值设为2，表示后续7行中至少有2行包含引文特征，这是一个强信号)\n",
    "            if is_heading_format and citation_features_count >= 5:\n",
    "                cut_index = i\n",
    "                # break  # 找到后立即停止向前的搜索\n",
    "    # 如果主搜索未找到，则启用备用搜索策略\n",
    "    if cut_index == -1:\n",
    "        # 在最后若干行中尝试识别高密度引文段\n",
    "        window_size = 10\n",
    "        min_citation_density = 9\n",
    "\n",
    "        # 从后往前滑动窗口检查\n",
    "        for i in range(len(lines) - window_size, start_search_index, -1):\n",
    "            window_lines = lines[i:i + window_size]\n",
    "            citation_features_count = 0\n",
    "\n",
    "            for line in window_lines:\n",
    "                stripped = line.strip()\n",
    "                if (re.search(r'\\((19|20)\\d{2}\\)', stripped) or\n",
    "                    re.search(r'\\b(19|20)\\d{2}[a-z]?\\b', stripped) or\n",
    "                    'doi:' in stripped.lower() or\n",
    "                    'et al' in stripped.lower() or\n",
    "                    re.search(r'^\\[\\d+\\]', stripped) or\n",
    "                    re.search(r'^\\d+\\.\\s', stripped) or\n",
    "                    re.search(r'[A-Z]\\.', stripped)):\n",
    "                    citation_features_count += 1\n",
    "\n",
    "            if citation_features_count >= min_citation_density:\n",
    "                cut_index = i\n",
    "    if cut_index != -1:\n",
    "        # 如果找到了切分点，返回它之前的所有内容\n",
    "        return '\\n'.join(lines[:cut_index]).strip(),'\\n'.join(lines[cut_index:]).strip()\n",
    "    \n",
    "    # 如果没有找到，返回原始文本（去除首尾空白）\n",
    "    return text.strip(), \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.807479Z",
     "iopub.status.busy": "2025-08-29T07:53:44.807157Z",
     "iopub.status.idle": "2025-08-29T07:53:44.830515Z",
     "shell.execute_reply": "2025-08-29T07:53:44.829601Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.807453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_data_availability_statement(content: str, content_type: str = 'text') -> tuple[str | None, str]:\n",
    "    \"\"\"\n",
    "    使用分层规则从学术文本中查找数据可用性声明 (DAS)。\n",
    "\n",
    "    Args:\n",
    "        content (str): 文献的完整内容，可以是纯文本或JATS XML格式的字符串。\n",
    "        content_type (str): 内容类型，'text' 或 'xml'。\n",
    "\n",
    "    Returns:\n",
    "        tuple[str | None, str]: 一个元组，包含找到的声明文本（或None）和使用的方法。\n",
    "    \"\"\"\n",
    "    # # --- 方法一：解析XML标签 (最高优先级) ---\n",
    "    # if content_type.lower() == 'xml':\n",
    "    #     try:\n",
    "    #         statement, method = _find_by_xml_tag(content)\n",
    "    #         if statement:\n",
    "    #             return statement, method\n",
    "    #         # 如果XML中没找到特定标签，将其转换为纯文本以进行后续方法\n",
    "    #         soup = BeautifulSoup(content, 'lxml-xml')\n",
    "    #         plain_text = soup.get_text(separator='\\n\\n', strip=True)\n",
    "    #     except Exception:\n",
    "    #         # 如果XML解析失败，则假定内容为纯文本\n",
    "    #         plain_text = content\n",
    "    # else:\n",
    "    #     plain_text = content\n",
    "\n",
    "    # --- 方法二：通过章节标题查找 (第二优先级) ---\n",
    "    statement_head, method_header = _find_by_section_header(content)\n",
    "\n",
    "    # --- 方法三：通过关键词和标识符搜索 (第三优先级) ---\n",
    "    statement_search, method_search = _find_by_keyword_search(content)\n",
    "    \n",
    "    if statement_head and statement_search:\n",
    "        return statement_search + '\\n' + statement_head,\"hybird\"\n",
    "    elif statement_head:\n",
    "        return statement_head, method_header\n",
    "    elif statement_search:\n",
    "        return statement_search, method_search\n",
    "\n",
    "    return None, \"Not Found\"\n",
    "def _find_by_section_header(plain_text: str) -> tuple[str | None, str]:\n",
    "    \"\"\"\n",
    "    通过正则表达式查找标准的DAS章节标题，并提取其后的固定长度文本。\n",
    "    此版本经过优化，可以处理标题与正文在同一行的情况。\n",
    "    \"\"\"\n",
    "    # 优化的正则表达式:\n",
    "    # 1. (?:^|\\n\\n): 匹配文本开头或一个新段落的开头，确保我们找到的是一个真正的标题。\n",
    "    # 2. \\s*: 匹配标题前的任何空白符。\n",
    "    # 3. ((data|...|le)|data\\s*deposition): 捕获核心的标题关键词。\n",
    "    # 4. [\\s.:;]*: 匹配标题后可能出现的空格、句号、冒号、分号，使其更灵活。\n",
    "    header_regex = re.compile(\n",
    "        r\"((?:data|code|software|materials)\\s*(?:and\\s+materials\\s*)?availab(?:ility|le)|data\\s*deposition)[\\s.:;]*\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # 在整个文本中搜索第一次出现的标题\n",
    "    match = header_regex.search(plain_text)\n",
    "    \n",
    "    if match:\n",
    "        # 找到匹配项的起始位置\n",
    "        start_index = match.start()\n",
    "        \n",
    "        # 从标题开始的位置，向后提取最多500个字符\n",
    "        # 使用 strip() 去除开头可能存在的换行符和空格\n",
    "        end_index = start_index + 500\n",
    "        full_statement = plain_text[start_index:end_index].strip()\n",
    "        \n",
    "        return full_statement, \"Section Header\"\n",
    "    \n",
    "    return None, \"Section Header\"\n",
    "\n",
    "def _find_by_keyword_search(plain_text: str) -> Tuple[Optional[str], str]:\n",
    "    \"\"\"在全文中搜索包含数据来源描述的句子，并应用长度和距离限制。\"\"\"\n",
    "    \n",
    "    # 数据来源关键词（包括 submitted, obtained from, GISAID 等）\n",
    "    # 数据来源关键词（“动作”或“通用概念”）- 精简版\n",
    "    data_source_keywords = re.compile(\n",
    "        r'\\b('\n",
    "        # --- 强一手信号：数据创建与章节标题 ---\n",
    "        r'data\\s+and\\s+materials\\s+availability|'\n",
    "        r'data\\s+availability|code\\s+availability|'\n",
    "        r'underlying\\s+data|source\\s+data|source\\s+code|'\n",
    "        r'generated|produced|collected|'\n",
    "        \n",
    "        # --- 常见一手/中性信号：提交与发布 ---\n",
    "        r'submitted|deposited|released|archived|'\n",
    "        r'data\\s+release|'\n",
    "        \n",
    "        # --- 中性可用性描述 ---\n",
    "        r'provided\\s+by|available\\s+at|available\\s+in|available\\s+from|'\n",
    "        r'can\\s+be\\s+found|are\\s+provided|are\\s+included|hosted|'\n",
    "        \n",
    "        # --- 中性核心名词 ---\n",
    "        r'dataset|datasets|data\\s+set|accession|availability'\n",
    "        r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # 数据存储库/标识符（用于确认是真实的数据来源）\n",
    "    repo_or_id_pattern = re.compile(\n",
    "        r'('\n",
    "        # --- Major Repository Names ---\n",
    "        r'\\b(zenodo|figshare|dryad|github|gitlab|GBIF)\\b|'\n",
    "        r'\\b(AlphaFold\\sDB|AlphaFold|ArrayExpress|'\n",
    "        r'BioImage\\sArchive|bia|BioModels|NCBI\\sBioProject|BioProject|NCBI\\sBioSample|BioSample|'\n",
    "        r'BRaunschweig\\sENzyme\\sDAtabase|BRENDA\\senzyme|BRENDA|BTO|'\n",
    "        r'cath-Gene3D|cath|Cellosaurus|ChEBI|ChEMBL|ClinicalTrials\\.gov|'\n",
    "        r'ComplexPortal|Coriell|'\n",
    "        r'database\\sof\\sgenotypes\\sand\\sphenotypes|dbgap|dbSNP|'\n",
    "        r'ebi\\smetagenomics|EBiSC|EMBL|EMDB|EMPIAR|Ensembl|'\n",
    "        r'European\\sGenome-phenome\\sArchive|EGA|'\n",
    "        r'European\\sNucleotide\\sArchive|EudraCT|'\n",
    "        r'Gene\\sExpression\\sOmnibus|GEO|Gene\\sOntology|GenBank|gisaid|gxa|'\n",
    "        r'HipSci|HUGO\\sGene\\sNomenclature\\sCommittee|hgnc|'\n",
    "        r'IntAct|InterPro|'\n",
    "        r'MetaboLights|molecular\\sinteraction\\sdatabase|MINT|'\n",
    "        r'Orphanet|Orphadata|ORPHA|'\n",
    "        r'Pangaea|Pfam|Protein\\sData\\sBank|PDB|ProteomeXchange|PRIDE|'\n",
    "        r'Reactome|Reference\\sSequence|RefSeq|rhea|RNAcentral|'\n",
    "        r'swiss-prot|swiss\\sprot|UniProtKB|TrEMBL|UniProt|Sprot|uniparc|'\n",
    "        r'TreeFam|1000\\sGenomes)\\b'\n",
    "        r')',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # 句子分割（改进版，处理缩写和复杂标点）\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s+', plain_text)\n",
    "    \n",
    "    matched_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_clean = sent.strip()\n",
    "        if not sent_clean:\n",
    "            continue\n",
    "            \n",
    "        # 规则1：句子必须包含两个关键词\n",
    "        # **改动1：获取匹配对象本身，而不是布尔值，以便后续计算位置**\n",
    "        source_match = data_source_keywords.search(sent_clean)\n",
    "        repo_match = repo_or_id_pattern.search(sent_clean)\n",
    "        \n",
    "        if source_match and repo_match:\n",
    "            # --- 新增逻辑：开始 ---\n",
    "\n",
    "            # **规则2：检查两个关键词之间的距离**\n",
    "            # 计算两个匹配项之间的距离（后一个的开头 - 前一个的结尾）\n",
    "            first_end = min(source_match.end(), repo_match.end())\n",
    "            second_start = max(source_match.start(), repo_match.start())\n",
    "            \n",
    "            # 修正：应该是后一个的开头 - 前一个的结尾\n",
    "            if source_match.start() < repo_match.start():\n",
    "                # source_match 在前\n",
    "                distance = repo_match.start() - source_match.end()\n",
    "            else:\n",
    "                # repo_match 在前\n",
    "                distance = source_match.start() - repo_match.end()\n",
    "\n",
    "            # 如果距离超过200个字符，则跳过这个句子\n",
    "            if distance > 200:\n",
    "                continue\n",
    "\n",
    "            # **规则3：如果句子太长，则进行精简**\n",
    "            final_snippet = sent_clean\n",
    "            if len(sent_clean) > 300:\n",
    "                # 确定两个关键词共同覆盖的核心区域\n",
    "                interest_start = min(source_match.start(), repo_match.start())\n",
    "                interest_end = max(source_match.end(), repo_match.end())\n",
    "                \n",
    "                # 计算核心区域的中点\n",
    "                midpoint = interest_start + (interest_end - interest_start) // 2\n",
    "                \n",
    "                # 以中点为中心，向两边扩展，总长度为200\n",
    "                snippet_start = max(0, midpoint - 150)\n",
    "                snippet_end = min(len(sent_clean), snippet_start + 300)\n",
    "\n",
    "                # 确保窗口调整后不会切断开头的单词\n",
    "                if snippet_start > 0:\n",
    "                   snippet_start = sent_clean.find(' ', snippet_start) + 1\n",
    "\n",
    "                snippet = sent_clean[snippet_start:snippet_end]\n",
    "                \n",
    "                # 如果截取了开头或结尾，则加上省略号\n",
    "                prefix = \"... \" if snippet_start > 0 else \"\"\n",
    "                suffix = \" ...\" if snippet_end < len(sent_clean) else \"\"\n",
    "                \n",
    "                final_snippet = f\"{prefix}{snippet}{suffix}\"\n",
    "\n",
    "            # --- 新增逻辑：结束 ---\n",
    "\n",
    "            matched_sentences.append(final_snippet)\n",
    "    \n",
    "    if matched_sentences:\n",
    "        return \"\\n\".join(matched_sentences), \"Data Source Detection\"\n",
    "    \n",
    "    return None, \"No data source mentions found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:53:44.831803Z",
     "iopub.status.busy": "2025-08-29T07:53:44.831477Z",
     "iopub.status.idle": "2025-08-29T07:53:45.219419Z",
     "shell.execute_reply": "2025-08-29T07:53:45.218624Z",
     "shell.execute_reply.started": "2025-08-29T07:53:44.831775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_doi = pd.read_csv('/kaggle/input/accession-ids-dataset/doi.csv')\n",
    "doi_series = df_doi[\"doi\"].dropna().astype(str)\n",
    "prefixes = doi_series.str.split('/', n=1).str[0]\n",
    "doi_prefix_set = set(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-29T08:35:39.243713Z",
     "iopub.status.busy": "2025-08-29T08:35:39.242675Z",
     "iopub.status.idle": "2025-08-29T08:48:53.23784Z",
     "shell.execute_reply": "2025-08-29T08:48:53.236873Z",
     "shell.execute_reply.started": "2025-08-29T08:35:39.243683Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "chunks_ref = []\n",
    "cover_information = []\n",
    "article_data_information = {}\n",
    "database_information = {}\n",
    "text_span_len = 100\n",
    "chunk_size = 200 \n",
    "overlap = 50\n",
    "found_items = []\n",
    "j = 0\n",
    "bad_ids = [\"10.5061/dryad\", \"10.6073/pasta\", \"10.5281/zenodo\", \"10.25386/genetics\", \"10.1175/jcli\"]\n",
    "journal_ids = [\n",
    "    \"10.18637/jss\", \"journal\",    \"10.1109/cvpr\", \"figshare\",   \"10.1111\", \"10.1016/j.\",\n",
    "    \"10.1002/\",   \"10.1038/\",     \"10.1126/\",   \"10.1073/\",   \"10.1093/\",   \"10.1103/\",\n",
    "    \"10.1021/\",   \"10.1007/\",     \"10.1186/\",   \"10.1371/\",   \"10.3390/\",   \"10.1155/\",\n",
    "    \"10.1159/\",   \"10.1161/\",     \"10.1210/\",   \"10.1212/\",   \"10.1214/\",   \"10.1215/\",\n",
    "    \"10.1242/\",   \"10.1256/\",     \"10.1261/\",   \"10.1289/\",   \"10.1299/\",   \"10.1300/\",\n",
    "    \"10.1310/\",   \"10.1320/\",     \"10.1330/\",   \"10.1340/\",   \"10.1350/\",   \"10.1360/\",\n",
    "    \"10.1370/\",   \"10.1380/\",     \"10.1390/\",   \"10.1400/\",   \"10.1410/\",   \"10.1420/\",\n",
    "    \"10.1430/\",   \"10.1440/\",     \"10.1450/\",   \"10.1460/\",   \"10.1470/\",   \"10.1480/\",\n",
    "    \"10.1490/\",   \"10.1500/\",     \"10.1510/\",   \"10.1520/\",   \"10.1530/\",   \"10.1540/\",\n",
    "    \"10.1550/\",   \"10.1560/\",     \"10.1570/\",   \"10.1580/\",   \"10.1590/\",   \"10.1600/\",\n",
    "    \"10.1610/\",   \"10.1620/\",     \"10.1630/\",   \"10.1640/\",   \"10.1650/\",   \"10.1660/\",\n",
    "    \"10.1670/\",   \"10.1680/\",     \"10.1690/\",   \"10.1700/\",   \"10.1710/\",   \"10.1720/\",\n",
    "    \"10.1730/\",   \"10.1740/\",     \"10.1750/\",   \"10.1760/\",   \"10.1770/\",   \"10.1780/\",\n",
    "    \"10.1790/\",   \"10.1800/\",     \"10.1810/\",   \"10.1820/\",   \"10.1830/\",   \"10.1840/\",\n",
    "    \"10.1850/\",   \"10.1860/\",     \"10.1870/\",   \"10.1880/\",   \"10.1890/\",   \"10.1900/\",\n",
    "    \"10.1910/\",   \"10.1920/\",     \"10.1930/\",   \"10.1940/\",   \"10.1950/\",   \"10.1960/\",\n",
    "    \"10.1970/\",   \"10.1980/\",     \"10.1990/\",   \"10.1080/\",   \"10.1029/\",   \"10.1143/\",\n",
    "    \"10.3238/arztebl\",\"10.3322/caac\",\"10.2458/azu_js_rc\",\"10.21105/joss\",\"10.1089\",\n",
    "    \"10.5194/gmd\",\"10.1146/annurev\",\"10.1098/rspb\",\"10.3897/neobiota\",\"10.3763/ghgmm\",\n",
    "    \"10.1175/jhm\",\"10.5194/acp\",\"10.5194/hess\",\"10.1098/\",\"10.11613/bm\",\"10.4161/rna\",\n",
    "    \"10.2307\",\"10.3354\",\"10.4159/harvard\",\"10.1051/forest\",\"10.1098/rstb\",\"10.1127\",\n",
    "    \"10.1086/\",\"10.1071/\",\"10.2193/\",\"10.1641/\",\"10.3201/\",\"10.1162/\",\"10.14806/ej\",\n",
    "    \"10.1669/\",\"10.1006/\",\"10.1152\",\"10.2525\",\"10.2217/\",\"10.14411\",\"10.14440/\",\n",
    "    \"10.5665/\",\"10.1113/\",\"10.1586/\",\"10.1669/\",\"10.1177/\",\"10.1895/\",\"10.13039/\",\n",
    "    \"10.1128/\",\"10.1191/\",\"10.1109\",\"10.4049/\",\"10.1145\",\"10.3389/fimmu\",\"10.1175/\",\n",
    "    \"10.2202/\",\"10.1175/\",\"10.4061/\",\n",
    "]\n",
    "def journal_is_in_match(match: str) -> bool:\n",
    "    if not match:\n",
    "        return False\n",
    "    match_lower = match.lower()\n",
    "    # 使用 any() 和生成器表达式，一旦找到匹配项就立即返回 True\n",
    "    return any(prefix in match_lower for prefix in journal_ids)\n",
    "with open(unique_datasets, 'r', encoding='utf-8') as f:\n",
    "    data_ids_doi = set(line.strip().lower() for line in f if line.strip().startswith(\"10.\"))\n",
    "data_ids, id_details= load_all_accession_ids_efficient()\n",
    "with open(reorganized_publication_dataset, 'r', encoding='utf-8') as f:\n",
    "    article_data = json.load(f)\n",
    "for filename in tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory))):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        j += 1\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        \n",
    "        # Extract article_id from filename\n",
    "        article_id = filename.split(\".pdf\")[0]\n",
    "        # if article_id.replace(\"_\",\"/\") not in article_data:\n",
    "        #     continue\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        cover = \"\"\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            if not cover:\n",
    "                cover = page_text\n",
    "            elif len(cover) < 1000:\n",
    "                cover += page_text\n",
    "            text += page_text + \"\\n\"\n",
    "            \n",
    "        doc.close()\n",
    "        # text, ref =split_text_and_references(text)\n",
    "        # print(text)\n",
    "        \n",
    "        text = normalize_text(text)\n",
    "        cover = normalize_text(cover)\n",
    "        text = add_period_to_heading_lines(text)\n",
    "        # ref = normalize_text(ref)\n",
    "        # text = re.sub(r'\\s+', ' ', content)\n",
    "        text = text.strip()\n",
    "        # text = re.sub(r\"[\\u200b\\u200c\\u200d\\uFEFF]\\n|[\\u200b\\u200c\\u200d\\uFEFF]\", \"\", text)\n",
    "        text = re.sub(r'<br>', '', text)\n",
    "        cover = re.sub(r'<br>', '', cover)\n",
    "        text = re.sub(r'(\\d+\\.)\\s+(\\d+)', r'\\1\\2', text)\n",
    "        text = re.sub(r'/\\s', '/', text)\n",
    "        # text = re.sub(r'\\n+', '\\n', text)\n",
    "        text = re.sub(r'\\\\_', '_', text)\n",
    "        text = fix_broken_links(text)\n",
    "        # print(text)\n",
    "        text = re.sub(r'(?<![\\.\\n])\\n', ' ', text)\n",
    "        # body_text = text\n",
    "        cover = re.sub(r'(?<![\\.\\n])\\n', ' ', cover)\n",
    "        cover_information.append((article_id,cover))\n",
    "        # full_text = text\n",
    "        body_text, ref = remove_references_section_v3(text)\n",
    "        data_availabilit,_ = find_data_availability_statement(text)\n",
    "        if data_availabilit:\n",
    "            article_data_information[article_id] = [data_availabilit]\n",
    "        else:\n",
    "            article_data_information[article_id] = []\n",
    "        # print(data_availabilit)\n",
    "        # print(body_text)\n",
    "\n",
    "        # print(ref)\n",
    "        # article_data = database_information.setdefault(article_id, {})\n",
    "        # for prefix, keyword_reg in DOI_KEYWORD_RULES.items():\n",
    "        #     keyword_matches = keyword_reg.finditer(body_text)\n",
    "        #     for match in keyword_matches:\n",
    "        #         context_list = article_data.setdefault(prefix, [])\n",
    "        #         start, end = match.start(), match.end()\n",
    "        #         window_start = max(0, start - 100)\n",
    "        #         window_end = min(len(body_text), end + 100)\n",
    "        #         context_window_text = body_text[window_start:window_end]\n",
    "        #         context_list.append(context_window_text.strip())\n",
    "        clustered_results = find_and_cluster_keywords(\n",
    "            body_text=body_text,\n",
    "            rules=DOI_KEYWORD_RULES,\n",
    "            window_size=100,\n",
    "            cluster_gap=50\n",
    "        )\n",
    "    \n",
    "        # 如果找到了去重后的结果，就将其更新到主字典中\n",
    "        if clustered_results:\n",
    "            database_information.setdefault(article_id, {}).update(clustered_results)\n",
    "\n",
    "        for pattern_name, compiled_pattern in patterns_to_find.items():\n",
    "                # matches = compiled_pattern.finditer(text)\n",
    "            if pattern_name == \"doi\" or pattern_name == \"doi2\":\n",
    "                target_context_before = 200  # 匹配结果前取300字符\n",
    "                target_context_after = 100   # 匹配结果后取200字符\n",
    "                text = body_text\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            elif pattern_name == \"doi_ref\" or pattern_name == \"doi_ref2\":\n",
    "                target_context_before = 200  # 匹配结果前取300字符\n",
    "                target_context_after = 100   # 匹配结果后取200字符\n",
    "                text = ref\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            # elif pattern_name == 'acc_id_ref':\n",
    "            #     target_context_before = 200  # 匹配结果前取300字符\n",
    "            #     target_context_after = 100   # 匹配结果后取200字符\n",
    "            #     text = ref\n",
    "            #     matches = compiled_pattern.finditer(text)\n",
    "            else:\n",
    "                target_context_before = 200  # 匹配结果前取300字符\n",
    "                target_context_after = 100   # 匹配结果后取200字符\n",
    "                text = body_text + \"\\n\" + ref\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            # matches = compiled_pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                match_text = match.group()\n",
    "                match_text = re.sub(r'PDB:?\\s*([1-9][A-Z0-9]{3})', r'\\1', match_text)\n",
    "                match_text = re.sub(r'\\s+', '', match_text)\n",
    "                match_text = re.sub(r'[^A-Za-z0-9]+$', '', match_text)\n",
    "                if not is_balanced(match_text):\n",
    "                    continue\n",
    "                start, end = match.start(), match.end()\n",
    "                \n",
    "                # 获取匹配项的上下文\n",
    "                context_start, context_end = find_context_boundaries(\n",
    "                    text, start, end, target_context_before, target_context_after\n",
    "                )\n",
    "                context_chunk = text[context_start:context_end].strip()\n",
    "                \n",
    "                # 特殊处理DOI\n",
    "                if pattern_name == 'doi' or pattern_name == 'doi_ref' or pattern_name == 'doi2' or pattern_name == 'doi_ref2':\n",
    "                    match_text = match_text.replace(r'[-.,;:!?\\/\\)\\]\\(\\[]+$', '')\n",
    "                    if match_text.lower().startswith(\"dryad.\"):\n",
    "                        match_text = \"10.5061/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"zenodo.\"):\n",
    "                        match_text = \"10.5281/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"pasta/\"):\n",
    "                        match_text = \"10.6073/\" + match_text.lower()\n",
    "#                      or journal_is_in_match(match_text)\n",
    "                    elif match_text.lower().startswith(\"pangaea.\"):\n",
    "                        match_text = \"10.1594/\" + match_text.lower()\n",
    "                    if article_id.split('_')[0] in match_text or match_text.lower() in bad_ids or journal_is_in_match(match_text) or len(match_text.lower().split('/')[-1])<4:\n",
    "                        continue  # 跳过不符合要求的DOI\n",
    "                    prefix = match_text.split(\"/\")[0]\n",
    "                    # print(prefix)\n",
    "                    if prefix not in doi_prefix_set:\n",
    "                        # print(\"不行\",match_text)\n",
    "                        continue\n",
    "                    result_value = 'https://doi.org/' + match_text.lower()\n",
    "                    if result_value not in data_ids_doi and match_text.lower() not in data_ids_doi:\n",
    "                        continue\n",
    "                else:\n",
    "                    result_value = match_text\n",
    "                    db_key = SPECIAL_MAPPINGS.get(pattern_name, pattern_name.split('_')[0])\n",
    "                    if result_value not in data_ids:\n",
    "                        continue\n",
    "                    elif db_key in CONTEXT_VALIDATION_RULES:\n",
    "                        window_size = CONTEXT_VALIDATION_RULES[db_key][\"window_size\"]\n",
    "                        window_start = max(0, start - window_size)\n",
    "                        window_end = min(len(text), end + window_size)\n",
    "                    \n",
    "                        # 提取上下文窗口的文本\n",
    "                        context_window_text = text[window_start:window_end]\n",
    "                        context_regex = CONTEXT_VALIDATION_RULES[db_key]['context_regex']\n",
    "                        if not context_regex.search(context_window_text):\n",
    "                            # print(\"1\")\n",
    "                            continue\n",
    "                if pattern_name == \"doi_ref\" or pattern_name == 'acc_id_ref' or pattern_name == 'doi_ref2':\n",
    "                    chunks_ref.append((\n",
    "                        article_id,\n",
    "                        context_chunk,\n",
    "                        result_value,\n",
    "                        pattern_name\n",
    "                    ))\n",
    "                else:\n",
    "                    chunks.append((\n",
    "                        article_id,\n",
    "                        context_chunk,\n",
    "                        result_value,\n",
    "                        pattern_name\n",
    "                    ))\n",
    "xml_files = []\n",
    "for filename in tqdm(os.listdir(xml_directory), total=len(os.listdir(xml_directory))):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(xml_directory, filename)\n",
    "        article_id = filename.split(\".xml\")[0]\n",
    "        xml_files.append(article_id)\n",
    "        text = xml2text(xml_path)\n",
    "        # text = re.sub(r\"[\\u200b\\u200c\\u200d\\uFEFF]\\n|[\\u200b\\u200c\\u200d\\uFEFF]\", \"\", text)\n",
    "        # text = re.sub(r'<br>', '', text)\n",
    "        # text = re.sub(r'(\\d+\\.)\\s+(\\d+)', r'\\1\\2', text)\n",
    "        # text = re.sub(r'/\\s+', '/', text)\n",
    "        # text = re.sub(r'\\n+', '\\n', text)\n",
    "        # text = re.sub(r'\\\\_', '_', text)\n",
    "        # print(text)\n",
    "        # body_text = text\n",
    "        body_text, ref = remove_references_section_v3(text)\n",
    "        full_text = text\n",
    "        body_text = fix_broken_links(body_text)\n",
    "        data_availabilit,_ = find_data_availability_statement(text)\n",
    "        if data_availabilit:\n",
    "            article_data_information[article_id].append(data_availabilit)\n",
    "        ref = fix_broken_links(ref)\n",
    "        # print(ref)\n",
    "        target_context_before = 200  # 匹配结果前取300字符\n",
    "        target_context_after = 100   # 匹配结果后取200字符\n",
    "        for pattern_name, compiled_pattern in patterns_to_find.items():\n",
    "                # matches = compiled_pattern.finditer(text)\n",
    "            if pattern_name == \"doi\" or pattern_name == \"doi2\":\n",
    "                text = body_text\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            elif pattern_name == \"doi_ref\" or pattern_name == \"doi_ref2\":\n",
    "                text = ref\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            # elif pattern_name == 'acc_id_ref':\n",
    "            #     text = ref\n",
    "            #     matches = compiled_pattern.finditer(text)\n",
    "            else:\n",
    "                # continue\n",
    "                text = body_text + \"\\n\\n\" + ref\n",
    "                matches = compiled_pattern.finditer(text)\n",
    "            # matches = compiled_pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                match_text = match.group()\n",
    "                match_text = re.sub(r'PDB:?\\s*([1-9][A-Z0-9]{3})', r'\\1', match_text)\n",
    "                match_text = re.sub(r'\\s+', '', match_text)\n",
    "                match_text = re.sub(r'[^A-Za-z0-9]+$', '', match_text)\n",
    "                if not is_balanced(match_text):\n",
    "                    continue\n",
    "                start, end = match.start(), match.end()\n",
    "                if pattern_name == \"doi_ref\" or pattern_name == 'acc_id_ref' or pattern_name == 'doi2' or pattern_name == 'doi_ref2':\n",
    "                    prev_newline = text.rfind('\\n', 0, start)\n",
    "                    if prev_newline == -1:  # If no newline found, start from beginning\n",
    "                        prev_newline = 0\n",
    "                    else:\n",
    "                        prev_newline += 1  # Start after the newline character\n",
    "                    prev_newline = max(prev_newline, start - target_context_before)\n",
    "                    # Find the next newline\n",
    "                    next_newline = text.find('\\n', end)\n",
    "                    if next_newline == -1:  # If no newline found, go to end of text\n",
    "                        next_newline = len(text)\n",
    "                    next_newline = min(next_newline,end + target_context_after)\n",
    "                    # Extract the line containing the match\n",
    "                    context_chunk = text[prev_newline:next_newline]\n",
    "                else:\n",
    "                    # 获取匹配项的上下文\n",
    "                    context_start, context_end = find_context_boundaries(\n",
    "                        text, start, end, target_context_before, target_context_after\n",
    "                    )\n",
    "                    extract_start = max(context_start, start - target_context_before)  # 往前300，但不能小于0\n",
    "                    extract_end = min(context_end, end + target_context_after)  # 往后200，但不能超过全文长度\n",
    "                    context_chunk = text[context_start:context_end].strip()\n",
    "                # 特殊处理DOI\n",
    "                if pattern_name == 'doi' or pattern_name == 'doi_ref' or pattern_name == 'doi2' or pattern_name == 'doi_ref2':\n",
    "                    match_text = match_text.replace(r'[-.,;:!?\\/\\)\\]\\(\\[]+$', '')\n",
    "                    if match_text.lower().startswith(\"dryad.\"):\n",
    "                        match_text = \"10.5061/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"zenodo.\"):\n",
    "                        match_text = \"10.5281/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"pasta/\"):\n",
    "                        match_text = \"10.6073/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"pangaea.\"):\n",
    "                        match_text = \"10.1594/\" + match_text.lower()\n",
    "                    if article_id.split('_')[0] in match_text or match_text.lower() in bad_ids or journal_is_in_match(match_text) or (len(match_text.lower().split('/')[-1])<4 and not match_text.startswith('10.25326')):\n",
    "                        continue  # 跳过不符合要求的DOI\n",
    "                    prefix = match_text.split(\"/\")[0]\n",
    "                    # print(prefix)\n",
    "                    if prefix not in doi_prefix_set:\n",
    "                        continue\n",
    "                    result_value = 'https://doi.org/' + match_text.lower()\n",
    "                    if result_value not in data_ids_doi and match_text.lower() not in data_ids_doi:\n",
    "                        continue\n",
    "                else:\n",
    "                    result_value = match_text\n",
    "                    db_key = SPECIAL_MAPPINGS.get(pattern_name, pattern_name.split('_')[0])\n",
    "                    if result_value not in data_ids:\n",
    "                        continue\n",
    "                    elif db_key in CONTEXT_VALIDATION_RULES:\n",
    "                        window_size = CONTEXT_VALIDATION_RULES[db_key][\"window_size\"]\n",
    "                        window_start = max(0, start - window_size)\n",
    "                        window_end = min(len(text), end + window_size)\n",
    "                    \n",
    "                        # 提取上下文窗口的文本\n",
    "                        context_window_text = text[window_start:window_end]\n",
    "                        context_regex = CONTEXT_VALIDATION_RULES[db_key]['context_regex']\n",
    "                        if not context_regex.search(context_window_text):\n",
    "                            continue\n",
    "                if pattern_name == \"doi_ref\" or pattern_name == 'acc_id_ref' or pattern_name == 'doi_ref2':\n",
    "                    chunks_ref.append((\n",
    "                        article_id,\n",
    "                        context_chunk,\n",
    "                        result_value,\n",
    "                        pattern_name\n",
    "                    ))\n",
    "                else:\n",
    "                    chunks.append((\n",
    "                        article_id,\n",
    "                        context_chunk,\n",
    "                        result_value,\n",
    "                        pattern_name\n",
    "                    ))\n",
    "valid_chunks = set()\n",
    "for article_id,context_chunk,result_value, pattern_name in chunks:\n",
    "    if (article_id,result_value) not in valid_chunks:\n",
    "        valid_chunks.add((article_id,result_value))\n",
    "for article_id,context_chunk,result_value, pattern_name in chunks_ref:\n",
    "    if (article_id,result_value) not in valid_chunks:\n",
    "        valid_chunks.add((article_id,result_value))\n",
    "for filename in tqdm(os.listdir(xml_directory), total=len(os.listdir(xml_directory))):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(xml_directory, filename)\n",
    "        article_id = filename.split(\".xml\")[0]\n",
    "        with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        target_context_before = 300  # 匹配结果前取300字符\n",
    "        target_context_after = 200   # 匹配结果后取200字符\n",
    "        for pattern_name, compiled_pattern in patterns_to_find.items():\n",
    "            matches = compiled_pattern.finditer(text)\n",
    "            # matches = compiled_pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                match_text = match.group()\n",
    "                match_text = re.sub(r'PDB:?\\s*([1-9][A-Z0-9]{3})', r'\\1', match_text)\n",
    "                match_text = re.sub(r'\\s+', '', match_text)\n",
    "                match_text = re.sub(r'[^A-Za-z0-9]+$', '', match_text)\n",
    "                if not is_balanced(match_text):\n",
    "                    continue\n",
    "                start, end = match.start(), match.end()\n",
    "                if pattern_name == \"doi_ref\" or pattern_name == 'acc_id_ref' or pattern_name == 'doi2' or pattern_name == 'doi_ref2':\n",
    "                    prev_newline = text.rfind('\\n', 0, start)\n",
    "                    if prev_newline == -1:  # If no newline found, start from beginning\n",
    "                        prev_newline = 0\n",
    "                    else:\n",
    "                        prev_newline += 1  # Start after the newline character\n",
    "                    prev_newline = max(prev_newline, start - target_context_before)\n",
    "                    # Find the next newline\n",
    "                    next_newline = text.find('\\n', end)\n",
    "                    if next_newline == -1:  # If no newline found, go to end of text\n",
    "                        next_newline = len(text)\n",
    "                    next_newline = min(next_newline,end + target_context_after)\n",
    "                    # Extract the line containing the match\n",
    "                    context_chunk = text[prev_newline:next_newline]\n",
    "                else:\n",
    "                    # 获取匹配项的上下文\n",
    "                    context_start, context_end = find_context_boundaries(\n",
    "                        text, start, end, target_context_before, target_context_after\n",
    "                    )\n",
    "                    extract_start = max(context_start, start - target_context_before)  # 往前300，但不能小于0\n",
    "                    extract_end = min(context_end, end + target_context_after)  # 往后200，但不能超过全文长度\n",
    "                    context_chunk = text[context_start:context_end].strip()\n",
    "                # 特殊处理DOI\n",
    "                if pattern_name == 'doi' or pattern_name == 'doi_ref' or pattern_name == 'doi2' or pattern_name == 'doi_ref2':\n",
    "                    match_text = match_text.replace(r'[-.,;:!?\\/\\)\\]\\(\\[]+$', '')\n",
    "                    if match_text.lower().startswith(\"dryad.\"):\n",
    "                        match_text = \"10.5061/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"zenodo.\"):\n",
    "                        match_text = \"10.5281/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"pasta/\"):\n",
    "                        match_text = \"10.6073/\" + match_text.lower()\n",
    "                    elif match_text.lower().startswith(\"pangaea.\"):\n",
    "                        match_text = \"10.1594/\" + match_text.lower()\n",
    "                    if article_id.split('_')[0] in match_text or match_text.lower() in bad_ids or journal_is_in_match(match_text) or (len(match_text.lower().split('/')[-1])<4 and not match_text.startswith('10.25326')):\n",
    "                        continue  # 跳过不符合要求的DOI\n",
    "                    prefix = match_text.split(\"/\")[0]\n",
    "                    if prefix not in doi_prefix_set:\n",
    "                        continue\n",
    "                    result_value = 'https://doi.org/' + match_text.lower()\n",
    "                    if result_value not in data_ids_doi and match_text.lower() not in data_ids_doi:\n",
    "                        continue\n",
    "                else:\n",
    "                    result_value = match_text\n",
    "                    db_key = SPECIAL_MAPPINGS.get(pattern_name, pattern_name.split('_')[0])\n",
    "                    if result_value not in data_ids:\n",
    "                        continue\n",
    "                    elif db_key in CONTEXT_VALIDATION_RULES:\n",
    "                        window_size = CONTEXT_VALIDATION_RULES[db_key][\"window_size\"] * 2\n",
    "                        window_start = max(0, start - window_size)\n",
    "                        window_end = min(len(text), end + window_size)\n",
    "                    \n",
    "                        # 提取上下文窗口的文本\n",
    "                        context_window_text = text[window_start:window_end]\n",
    "                        context_regex = CONTEXT_VALIDATION_RULES[db_key]['context_regex']\n",
    "                        if not context_regex.search(context_window_text):\n",
    "                            # print(\"1\")\n",
    "                            continue\n",
    "                if (article_id,result_value) in valid_chunks:\n",
    "                    continue\n",
    "                valid_chunks.add((article_id,result_value))\n",
    "                chunks.append((\n",
    "                    article_id,\n",
    "                    context_chunk,\n",
    "                    result_value,\n",
    "                    pattern_name\n",
    "                ))\n",
    "# chunks = chunks + chunks_ref\n",
    "\n",
    "chunks_np = np.array(chunks)\n",
    "np.savetxt('/kaggle/working/chunks.txt', chunks_np, delimiter=',') # Saves as comma-separated values\n",
    "chunks_processed = []\n",
    "for article_id, context_chunk, result_value, pattern_name in chunks:\n",
    "    if result_value.startswith('https://doi.org/'):\n",
    "        chunks_processed.append((article_id, context_chunk, result_value, pattern_name))\n",
    "    elif article_id in xml_files:\n",
    "        chunks_processed.append((article_id, context_chunk, result_value, pattern_name))\n",
    "chunks = chunks_processed\n",
    "print(f\"\\n在所有文件中总共找到了 {len(chunks)} 个匹配项。\")\n",
    "\n",
    "# # 打印前5个找到的结果作为示例\n",
    "# for item in chunks_ref[:60]:\n",
    "# #     # if not item[2].startswith('https://doi.org/'):\n",
    "# #     #     continue\n",
    "#     print(f\"ID: {item[0]}, 文本: '{item[1]}', 匹配结果: {item[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask LLM to classify DOI links\n",
    "Use logits-processor-zoo MultipleChoiceLogitsProcessor to enforce LLM choose between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:07:33.331666Z",
     "iopub.status.busy": "2025-08-29T08:07:33.331348Z",
     "iopub.status.idle": "2025-08-29T08:07:33.337171Z",
     "shell.execute_reply": "2025-08-29T08:07:33.336205Z",
     "shell.execute_reply.started": "2025-08-29T08:07:33.331643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('10.5281' not in doi_prefix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T09:00:12.175164Z",
     "iopub.status.busy": "2025-08-29T09:00:12.173703Z",
     "iopub.status.idle": "2025-08-29T09:00:12.277378Z",
     "shell.execute_reply": "2025-08-29T09:00:12.276596Z",
     "shell.execute_reply.started": "2025-08-29T09:00:12.175126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_chunks_by_majority_id(chunks, id_details_dict):\n",
    "    \"\"\"\n",
    "    根据每组 article_id 中多数派的 PMCID 或 EXTID 过滤 chunks。\n",
    "    修改：保留 result_value 以 https 开头的项，且不对其进行过滤。\n",
    "\n",
    "    Args:\n",
    "        chunks: 包含 (article_id, context_chunk, result_value, pattern_name) 元组的列表。\n",
    "        id_details_dict: 由 load_all_accession_ids_efficient 返回的 id_details 字典。\n",
    "                        结构: {id: {'pmc_ids': set(), 'ext_ids': set()}}\n",
    "\n",
    "    Returns:\n",
    "        list: 过滤后的 chunks 列表。\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    # 1. 按 article_id 分组 (但这次不预先过滤 HTTPS)\n",
    "    grouped_chunks = defaultdict(list)\n",
    "    https_chunks = [] # 新增：用于存储 HTTPS 开头的 chunks\n",
    "    \n",
    "    for item in chunks:\n",
    "        article_id, context_chunk, result_value, pattern_name = item\n",
    "        # 检查 result_value 是否以 https 开头\n",
    "        if result_value and isinstance(result_value, str) and result_value.lower().startswith(\"https\"):\n",
    "            # 如果是，添加到专门的列表中\n",
    "            https_chunks.append(item)\n",
    "            # print(f\"暂存 HTTPS 项: {item}\")\n",
    "        else:\n",
    "            # 如果不是 HTTPS 开头，则按 article_id 分组 (用于后续过滤)\n",
    "            grouped_chunks[article_id].append(item)\n",
    "\n",
    "    filtered_chunks = []\n",
    "    # 首先，将所有 HTTPS 开头的 chunks 添加到结果中\n",
    "    filtered_chunks.extend(https_chunks)\n",
    "    print(f\"保留了 {len(https_chunks)} 个 result_value 以 'https' 开头的项。\")\n",
    "\n",
    "    processed_groups_count = 0\n",
    "\n",
    "    # 2. 遍历每个需要处理的分组 (非 HTTPS 的项)\n",
    "    for article_id, group_items in grouped_chunks.items():\n",
    "        if not group_items:\n",
    "            continue\n",
    "\n",
    "        # 3. 收集该组所有 result_value 对应的 PMCID 和 EXTID\n",
    "        all_pmcids = []\n",
    "        all_extids = []\n",
    "        item_details_map = {} # 临时存储每个 item 及其对应的 details\n",
    "\n",
    "        for item in group_items:\n",
    "            _, _, result_value, _ = item\n",
    "            # 使用 result_value 查找详细信息\n",
    "            details = id_details_dict.get(result_value, {'pmc_ids': set(), 'ext_ids': set()})\n",
    "            item_details_map[item] = details\n",
    "            all_pmcids.extend(list(details['pmc_ids']))\n",
    "            all_extids.extend(list(details['ext_ids']))\n",
    "\n",
    "        # 4. 统计并找出出现频率最高的 PMCID 和 EXTID\n",
    "        pmcid_counter = Counter(all_pmcids)\n",
    "        extid_counter = Counter(all_extids)\n",
    "\n",
    "        most_common_pmcid = pmcid_counter.most_common(1)[0][0] if pmcid_counter else None\n",
    "        most_common_extid = extid_counter.most_common(1)[0][0] if extid_counter else None\n",
    "\n",
    "        # 如果没有找到任何 PMCID 或 EXTID，保留所有该组的非 HTTPS 项\n",
    "        if most_common_pmcid is None and most_common_extid is None:\n",
    "            print(f\"警告: 分组 {article_id} 中没有找到任何关联的 PMCID 或 EXTID，保留所有非-HTTPS 项。\")\n",
    "            filtered_chunks.extend(group_items)\n",
    "            processed_groups_count += 1\n",
    "            continue\n",
    "\n",
    "        # print(f\"分组 {article_id} - 最频繁 PMCID: {most_common_pmcid}\")\n",
    "        # print(f\"分组 {article_id} - 最频繁 EXTID: {most_common_extid}\")\n",
    "\n",
    "        # 5. 再次遍历该组，根据多数派 ID 进行过滤 (仅针对非 HTTPS 项)\n",
    "        group_kept_items = []\n",
    "        for item in group_items:\n",
    "            details = item_details_map.get(item, {'pmc_ids': set(), 'ext_ids': set()})\n",
    "            item_pmcids = details['pmc_ids']\n",
    "            item_extids = details['ext_ids']\n",
    "\n",
    "            # 判断保留条件\n",
    "            keep_item = False\n",
    "            if most_common_pmcid and most_common_pmcid in item_pmcids:\n",
    "                keep_item = True\n",
    "            elif most_common_extid and most_common_extid in item_extids:\n",
    "                keep_item = True\n",
    "\n",
    "            if keep_item:\n",
    "                group_kept_items.append(item)\n",
    "\n",
    "        # print(f\"分组 {article_id} 处理完成，保留 {len(group_kept_items)} / {len(group_items)} 个非-HTTPS 项。\")\n",
    "        filtered_chunks.extend(group_kept_items)\n",
    "        processed_groups_count += 1\n",
    "\n",
    "    print(f\"总共处理了 {processed_groups_count} 个需要过滤的 article_id 分组。\")\n",
    "    print(f\"过滤后剩余 chunks 总数: {len(filtered_chunks)} (原始总数: {len(chunks)})\")\n",
    "    return filtered_chunks\n",
    "chunks = filter_chunks_by_majority_id(chunks, id_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:07:33.441095Z",
     "iopub.status.busy": "2025-08-29T08:07:33.440817Z",
     "iopub.status.idle": "2025-08-29T08:07:33.445297Z",
     "shell.execute_reply": "2025-08-29T08:07:33.444389Z",
     "shell.execute_reply.started": "2025-08-29T08:07:33.441073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, item in enumerate(chunks):\n",
    "#     article_id, academic_text, dataset_id,_ = item\n",
    "#     data_information = article_data_information[article_id]\n",
    "#     if data_information:\n",
    "#         # print(data_information)\n",
    "#         # print(data_information)\n",
    "#         data_information = ''.join(data_information)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:07:33.446556Z",
     "iopub.status.busy": "2025-08-29T08:07:33.4462Z",
     "iopub.status.idle": "2025-08-29T08:07:33.464026Z",
     "shell.execute_reply": "2025-08-29T08:07:33.46311Z",
     "shell.execute_reply.started": "2025-08-29T08:07:33.446536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import csv\n",
    "# label_df = pd.read_csv(labels_dir)\n",
    "# label_df_new = pd.read_csv(\"/root/kk/new_training_labels (1).csv\")\n",
    "# label_df = pd.concat([label_df, label_df_new]).drop_duplicates()\n",
    "# label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "# valid_article_ids = set(label_df['article_id'])\n",
    "# label_dict = {(row['article_id'], row['dataset_id']): row['type']\n",
    "#               for _, row in label_df.iterrows()}\n",
    "# extended_chunks = []\n",
    "# skipped_count = 0\n",
    "\n",
    "# for article_id, chunk, dataset_id, pattern_name in chunks + chunks_ref:\n",
    "#     if article_id not in valid_article_ids:\n",
    "#         skipped_count += 1\n",
    "#         continue  # 使用 continue 关键字跳到下一个循环\n",
    "#     type_value = label_dict.get((article_id, dataset_id), None)\n",
    "#     record = {\n",
    "#         'article_id': article_id,\n",
    "#         'chunk': chunk,\n",
    "#         'dataset_id': dataset_id,\n",
    "#         'type': type_value, \n",
    "#         'pattern_name': pattern_name\n",
    "#     }\n",
    "#     extended_chunks.append(record)\n",
    "\n",
    "# output_file = 'output.json'\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(extended_chunks, f, ensure_ascii=False, indent=2)\n",
    "# df = pd.DataFrame(extended_chunks)\n",
    "# output_file = 'output.csv'\n",
    "# df.to_csv(\n",
    "#     output_file,\n",
    "#     index=False,\n",
    "#     encoding='utf-8',\n",
    "#     quoting=csv.QUOTE_MINIMAL,  # 只在必要时加引号\n",
    "#     escapechar='\\\\'\n",
    "# )\n",
    "# print(\"处理完成。\")\n",
    "# if skipped_count > 0:\n",
    "#     print(f\"跳过了 {skipped_count} 条 article_id 不在标签文件中的记录。\")\n",
    "# print(f\"最终 {len(extended_chunks)} 条有效数据已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:07:33.465299Z",
     "iopub.status.busy": "2025-08-29T08:07:33.465016Z",
     "iopub.status.idle": "2025-08-29T08:07:33.489309Z",
     "shell.execute_reply": "2025-08-29T08:07:33.488168Z",
     "shell.execute_reply.started": "2025-08-29T08:07:33.465275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.DataFrame(author_information)\n",
    "# output_file = 'author_information.csv'\n",
    "# df.to_csv(\n",
    "#     output_file,\n",
    "#     index=False,\n",
    "#     encoding='utf-8',\n",
    "#     quoting=csv.QUOTE_MINIMAL,  # 只在必要时加引号\n",
    "#     escapechar='\\\\'\n",
    "# )\n",
    "# output_file = 'author_information.json'\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(author_information, f, ensure_ascii=False, indent=2)\n",
    "# output_file = 'article_data_information.json'\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(article_data_information, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T09:00:21.405913Z",
     "iopub.status.busy": "2025-08-29T09:00:21.405583Z",
     "iopub.status.idle": "2025-08-29T09:00:21.435343Z",
     "shell.execute_reply": "2025-08-29T09:00:21.434333Z",
     "shell.execute_reply.started": "2025-08-29T09:00:21.405888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     label_df = pd.read_csv(labels_dir)\n",
    "#     label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "    \n",
    "#     # 提取数据\n",
    "#     true_values = set(label_df['dataset_id'].astype(str).unique())\n",
    "#     detected_values = {str(item[2]) for item in chunks + chunks_ref}\n",
    "#     # print(f\"\\n在所有文件中总共找到了 {len(detected_values)} 个匹配项。\")\n",
    "#     # 计算差异\n",
    "#     extra_detections = detected_values - true_values\n",
    "#     missing_detections = true_values - detected_values\n",
    "    \n",
    "#     # 输出结果\n",
    "#     print(f\"多出的检测结果数量: {len(extra_detections)}\")\n",
    "#     if extra_detections:\n",
    "#         print(\"多出的内容示例:\", list(extra_detections)[:100])  # 打印前5个示例\n",
    "    \n",
    "#     print(f\"\\n缺少的检测结果数量: {len(missing_detections)}\")\n",
    "#     if missing_detections:\n",
    "#         print(\"缺少的内容示例:\", list(missing_detections)[:100])\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    label_df = pd.read_csv(labels_dir)\n",
    "    # 过滤掉 'Missing' 类型的标签\n",
    "    label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "    # 1. 提取标签数据中的 (article_id, dataset_id) pairs\n",
    "    # 确保 ID 是字符串类型，处理可能的 NaN 值\n",
    "    label_df['article_id_str'] = label_df['article_id'].fillna('NaN').astype(str)\n",
    "    label_df['dataset_id_str'] = label_df['dataset_id'].fillna('NaN').astype(str)\n",
    "    \n",
    "    # 创建一个包含所有正确 (article_id, dataset_id) pairs 的集合\n",
    "    # 使用元组 (article_id, dataset_id) 作为集合元素\n",
    "    true_pairs = set(zip(label_df['article_id_str'], label_df['dataset_id_str']))\n",
    "    # 或者如果你想确保 dataset_id 不是 'Missing' 且两个 ID 都存在，可以在上面的过滤后进行：\n",
    "    # true_pairs = set(zip(label_df['article_id'].astype(str), label_df['dataset_id'].astype(str)))\n",
    "\n",
    "    # 2. 提取检测结果中的 (article_id, dataset_id) pairs\n",
    "    # 假设 chunks 和 chunks_ref 中的元素结构是 (start, end, dataset_id, article_id)\n",
    "    # 你需要根据实际结构调整索引 item[2] 是 dataset_id, item[3] 是 article_id\n",
    "    # 同样确保它们是字符串\n",
    "    try:\n",
    "        detected_pairs = {(str(item[0]), str(item[2])) for item in chunks + chunks_ref} # (article_id, dataset_id)\n",
    "    except IndexError:\n",
    "        print(\"警告：chunks 或 chunks_ref 中的元素结构与预期不符，无法提取 (article_id, dataset_id) pair。\")\n",
    "        detected_pairs = set()\n",
    "    # 如果结构是 (dataset_id, article_id, ...), 则使用:\n",
    "    # detected_pairs = {(str(item[1]), str(item[0])) for item in chunks + chunks_ref}\n",
    "    \n",
    "    print(f\"\\n在所有文件中总共找到了 {len(detected_pairs)} 个 (article_id, dataset_id) 匹配项。\")\n",
    "\n",
    "    # 3. 计算差异\n",
    "    # 多出的检测结果：在 detected 中但不在 true 中\n",
    "    extra_detections = detected_pairs - true_pairs\n",
    "    # 缺少的检测结果：在 true 中但不在 detected 中\n",
    "    missing_detections = true_pairs - detected_pairs\n",
    "\n",
    "    # 4. 输出结果\n",
    "    print(f\"多出的检测结果数量 (False Positives): {len(extra_detections)}\")\n",
    "    if extra_detections:\n",
    "        # 打印前100个多出的 pairs 作为示例\n",
    "        # extra_detections 是一个元组集合，可以直接转换为字符串列表\n",
    "        sorted_extra = sorted(list(extra_detections))\n",
    "        print(\"多出的内容示例 (article_id, dataset_id):\", \"\\n\".join([str(pair) for pair in list(sorted_extra)[:400]]))\n",
    "\n",
    "    print(f\"\\n缺少的检测结果数量 (False Negatives): {len(missing_detections)}\")\n",
    "    if missing_detections:\n",
    "        # 打印前100个缺少的 pairs 作为示例\n",
    "        sorted_missing = sorted(list(missing_detections))\n",
    "        print(\"缺少的内容示例 (article_id, dataset_id):\", \"\\n\".join([str(pair) for pair in list(sorted_missing)[:100]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:09:44.80785Z",
     "iopub.status.busy": "2025-08-29T08:09:44.806774Z",
     "iopub.status.idle": "2025-08-29T08:09:44.816782Z",
     "shell.execute_reply": "2025-08-29T08:09:44.815675Z",
     "shell.execute_reply.started": "2025-08-29T08:09:44.807819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MODEL_DIRECTORY = '/kaggle/input/scibert-finetuning/scibert_classifier_model_balanced_0710'\n",
    "# classifier = BatchCitationClassifier(model_path=MODEL_DIRECTORY)\n",
    "# prediction_results = classifier.predict_batch(chunks)\n",
    "# print(\"\\n\" + \"=\"*20 + \" 批量预测结果 \" + \"=\"*20)\n",
    "# answers = [None] * len(chunks)\n",
    "# for i, result in enumerate(prediction_results):\n",
    "#     answers[i] = result['predicted_label'] if result['predicted_label'] != \"Unknown\" else None\n",
    "# print(\"\\n\" + \"=\"*55)\n",
    "# del classifier\n",
    "# torch.cuda.empty_cache()\n",
    "# import gc\n",
    "# gc.collect()\n",
    "answers = [\"Secondary\"] * len(chunks)\n",
    "# chunks_ref_pro = []\n",
    "# body_length = len(chunks)\n",
    "# for i in range(body_length):\n",
    "#     if chunks[i][2].startswith(\"https://doi.org/\"):\n",
    "#         answers[i] = \"Secondary\"\n",
    "# chunks.extend(chunks_ref)\n",
    "# answers.extend([\"Secondary\"] * len(chunks_ref))\n",
    "body_length = len(chunks)\n",
    "for i in range(body_length):\n",
    "    if chunks[i][2].startswith(\"https://doi.org/\"):\n",
    "        answers[i] = \"Secondary\"\n",
    "    elif chunks[i][2].startswith(\"SAMN\"):\n",
    "        answers[i] = \"Primary\"\n",
    "    else:\n",
    "        answers[i] = \"Secondary\"\n",
    "chunks.extend(chunks_ref)\n",
    "answers.extend([\"Secondary\"] * len(chunks_ref))\n",
    "# for article_id, academic_text, dataset_id,pattern_name in chunks_ref:\n",
    "#     chunks_ref_pro.append((article_id, academic_text, dataset_id,pattern_name))\n",
    "#     answers.append(\"Secondary\")\n",
    "# chunks = chunks + chunks_ref_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, item in enumerate(chunks):\n",
    "#     if item[2].startswith(\"https://doi.org/\"):\n",
    "#         if item[2].startswith(\"https://doi.org/10.5061/\") or item[2].startswith(\"https://doi.org/10.6073/\"):\n",
    "#             answers[i] = \"Primary\"\n",
    "#     elif item[2].startswith(\"SAMN\"):\n",
    "#         answers[i] = \"Primary\"\n",
    "#     else:\n",
    "#         answers[i] = \"Secondary\"\n",
    "# chunks = chunks + chunks_ref\n",
    "# for i, item in enumerate(chunks_ref):\n",
    "#     answers.append(\"Secondary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if LOCAL:\n",
    "#     model_path = \"/root/autodl-tmp/Qwen2.5-32B-Insturct-AWQ\"\n",
    "#     # model_path = \"/kaggle/input/qwen3-30b-a3b-instruct-2507-awq/transformers/default/1\"\n",
    "# else:\n",
    "#     model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# llm = vllm.LLM(\n",
    "#     model_path,\n",
    "#     # quantization='awq',\n",
    "#     tensor_parallel_size=torch.cuda.device_count(),\n",
    "#     gpu_memory_utilization=0.95,\n",
    "#     trust_remote_code=True,\n",
    "#     dtype=\"half\",\n",
    "#     enforce_eager=True,\n",
    "#     max_model_len=4096,\n",
    "#     disable_log_stats=True,\n",
    "#     enable_prefix_caching=True\n",
    "# )\n",
    "# tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for article_id, chunks in chunked_articles.items():\n",
    "#     print(f\"\\n文章ID: {article_id}\")\n",
    "#     print(f\"PDF分块数: {len(chunks['pdf_chunks'])}\")\n",
    "#     if chunks['xml_chunks']:\n",
    "#         print(f\"XML分块数: {len(chunks['xml_chunks'])}\")\n",
    "#     print(\"第一个PDF块:\", chunks['pdf_chunks'][0][:50] + \"...\")  # 打印前50个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if LOCAL:\n",
    "    model_path = \"/root/autodl-tmp/Qwen2.5-32B-Insturct-AWQ\"\n",
    "    # model_path = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1\"\n",
    "    # model_path = \"/kaggle/input/qwen3-30b-a3b-instruct-2507-awq/transformers/default/1\"\n",
    "else:\n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    # quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.95,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=4096,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def split_text_with_overlap(text, chunk_size=500, overlap_size=100):\n",
    "#     \"\"\"\n",
    "#     将文本分块，带有重叠区域\n",
    "#     :param text: 输入文本\n",
    "#     :param chunk_size: 每块大小(字符数)\n",
    "#     :param overlap_size: 重叠区域大小(字符数)\n",
    "#     :return: 分块后的文本列表\n",
    "#     \"\"\"\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     end = chunk_size\n",
    "#     text_length = len(text)\n",
    "    \n",
    "#     while start < text_length:\n",
    "#         chunk = text[start:end]\n",
    "#         chunks.append(chunk)\n",
    "#         start += (chunk_size - overlap_size)\n",
    "#         end = start + chunk_size\n",
    "#         # 确保最后一块不会超出文本长度\n",
    "#         if end > text_length:\n",
    "#             end = text_length\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# def process_article_data(article_data_information, chunk_size=500, overlap_size=100):\n",
    "#     \"\"\"\n",
    "#     处理文章数据，生成分块\n",
    "#     :param article_data_information: 原始文章数据 {article_id: [pdf_text, xml_text(可选)]}\n",
    "#     :param chunk_size: 每块大小\n",
    "#     :param overlap_size: 重叠区域大小\n",
    "#     :return: 分块后的数据 {article_id: {'pdf_chunks': [], 'xml_chunks': []}}\n",
    "#     \"\"\"\n",
    "#     chunked_data = {}\n",
    "    \n",
    "#     for article_id, texts in article_data_information.items():\n",
    "#         pdf_text = texts[0]  # PDF文本一定存在\n",
    "#         xml_text = texts[1] if len(texts) > 1 else None  # XML文本可能不存在\n",
    "        \n",
    "#         # 处理PDF文本\n",
    "#         pdf_chunks = split_text_with_overlap(pdf_text, chunk_size, overlap_size)\n",
    "        \n",
    "#         # 处理XML文本(如果存在)\n",
    "#         xml_chunks = split_text_with_overlap(xml_text, chunk_size, overlap_size) if xml_text else None\n",
    "        \n",
    "#         chunked_data[article_id] = pdf_chunks.extend(xml_chunks)\n",
    "    \n",
    "#     return chunked_data\n",
    "\n",
    "# # # 使用示例\n",
    "# # article_data_information = {\n",
    "# #     \"article1\": [\"这是从PDF解析的长文本...\" * 100],  # 只有PDF\n",
    "# #     \"article2\": [\"这是从PDF解析的长文本...\" * 100, \"这是从XML解析的长文本...\" * 100]  # PDF和XML\n",
    "# # }\n",
    "\n",
    "# chunked_articles = process_article_data(article_data_information)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SYS_PROMPT_ACCESSSION = \"\"\"\n",
    "# You are a highly accurate accession ID validator. Given a text snippet and a candidate ID, you must determine if the ID is a valid research dataset accession number. Your judgment for classifying as 'B' must be very strict.\n",
    "\n",
    "# Choose one of the following:\n",
    "#  A) Valid Dataset Accession ID — The ID is a plausible dataset accession number, and there is no strong, explicit evidence proving it is something else.\n",
    "#  B) Not a Dataset Accession ID — There is explicit and unambiguous context proving the ID is a literature citation, a classification code, or a product number.\n",
    "\n",
    "# === Step-by-Step Logic (Strict Order) ===\n",
    "# 1.  **First, Search for Explicit NON-Dataset Evidence.** Your primary task is to find clear proof that the ID is **NOT** a dataset. Read the text surrounding the ID. If, and only if, the context provides unambiguous proof, you will classify it as **B**. Look for these definitive clues:\n",
    "#     -   **Literature Context:** The ID is clearly part of a full literature citation, appearing next to a journal name (`Geophys Res Lett`), volume number (`33:`), or in a formal reference.\n",
    "#     -   **Classification Context:** The text explicitly names the ID type with keywords like \"EC number\", \"KEGG ID\", \"CAS number\".\n",
    "#     -   **Product Context:** The text explicitly mentions manufacturers (`Abcam`, `Sigma-Aldrich`) or product types (`antibody`, `reagent`, `catalog no.`).\n",
    "\n",
    "# 2.  **Default to A (Valid Dataset Accession ID).** If you cannot find any of the explicit non-dataset clues listed in Step 1, you **MUST** classify the ID as **A**.\n",
    "#     -   This rule applies even if the context is neutral or ambiguous.\n",
    "#     -   This rule applies even if the ID's format is unfamiliar. The burden of proof is on proving the ID is *not* a dataset.\n",
    "\n",
    "# === Few-Shot Examples ===\n",
    "# 1) **Case: Explicit Literature Identifier.**\n",
    "#    - `Text`: \"Ralph FM... (2006) Flooding on California’s Russian River... Geophys Res Lett 33:L13801. https://doi.org/10.1029/2006GL026689\"\n",
    "#    - `ID to Classify`: `L13801`\n",
    "#    - → **B** (Reasoning: There is explicit proof. The ID is part of a literature citation for the journal \"Geophys Res Lett\".)\n",
    "\n",
    "# 2) **Case: Explicit Biological Classification.**\n",
    "#    - `Text`: \"Nineteen different four-level EC numbers are assigned to this family in Gene3D: 2.1.1.14... and 6.3.5.2.\"\n",
    "#    - `ID to Classify`: `6.3.5.2`\n",
    "#    - → **B** (Reasoning: There is explicit proof. The text identifies this as an \"EC number.\")\n",
    "\n",
    "# 3) **Case: Ambiguous ID, Defaulting to A.**\n",
    "#    - `Text`: \"The analysis was performed on sample C1035, which showed high expression.\"\n",
    "#    - `ID to Classify`: `C1035`\n",
    "#    - → **A** (Reasoning: There is no explicit proof this is a non-dataset ID. The context is neutral. According to Rule #2, it defaults to A.)\n",
    "\n",
    "# 4) **Case: Explicit Product Catalog Number.**\n",
    "#    - `Text`: \"We used a mouse anti-BRD3 antibody (1:100; ab50818, Abcam).\"\n",
    "#    - `ID to Classify`: `ab50818`\n",
    "#    - → **B** (Reasoning: There is explicit proof. The context \"antibody\" and manufacturer \"Abcam\" confirms it's a product.)\n",
    "\n",
    "# 5) **Case: Neutral Context, Defaulting to A.**\n",
    "#    - `Text`: \"Sequence reads were submitted to NCBI under BioProject accession PRJNA765432.\"\n",
    "#    - `ID to Classify`: `PRJNA765432`\n",
    "#    - → **A** (Reasoning: There is no evidence this is a non-dataset ID, so it is classified as A.)\n",
    "\n",
    "# === Instructions ===\n",
    "# - Be very strict about classifying as B. You need undeniable proof.\n",
    "# - In cases of doubt, neutrality, or ambiguity, always default to A.\n",
    "# - Output exactly one letter: A or B.\n",
    "# \"\"\".strip()\n",
    "# prompts = []\n",
    "# for i, item in enumerate(chunks):\n",
    "#     article_id, academic_text, dataset_id,pattern_name = item\n",
    "#     # article_id, academic_text = item\n",
    "#     # for ref in items[i]:\n",
    "#     if dataset_id.startswith(\"https://doi.org/\"):\n",
    "#         continue\n",
    "#     elif pattern_name not in bad_ids_acc:\n",
    "#         continue \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSSION},\n",
    "#         {\"role\": \"user\", \"content\": f\"Text:\\n{academic_text}\\nID to Classify: {dataset_id}\"}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         tokenize=False,\n",
    "#         enable_thinking=False,\n",
    "#     )\n",
    "#     prompts.append(prompt)\n",
    "# mclp = MultipleChoiceLogitsProcessor(tokenizer, \n",
    "#                                      choices=[\"A\", \"B\"])\n",
    "\n",
    "\n",
    "# outputs = llm.generate(\n",
    "#     prompts,\n",
    "#     vllm.SamplingParams(\n",
    "#         seed=777,\n",
    "#         temperature=0.1,\n",
    "#         skip_special_tokens=True,\n",
    "#         max_tokens=1,\n",
    "#         logits_processors=[mclp],\n",
    "#         logprobs=len(mclp.choices)\n",
    "\n",
    "#     ),\n",
    "#     use_tqdm=True\n",
    "# )\n",
    "# logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "# choices = [max(d, key=d.get) for d in logprobs]\n",
    "# idx = 0\n",
    "# for i, item in enumerate(chunks):\n",
    "#     if not item[2].startswith(\"https://doi.org/\"):\n",
    "#         if item[3] not in bad_ids_acc:\n",
    "#             continue\n",
    "#         if choices[idx] == 'B':\n",
    "#             # print(item[1])\n",
    "#             # print(item[2])\n",
    "#             answers[i] = None\n",
    "#             idx += 1\n",
    "#         else:\n",
    "#             answers[i] = \"Secondary\"\n",
    "#             idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYS_PROMPT_AUTHOR_INFORMATION = \"\"\"\n",
    "You are a highly efficient and precise author extraction tool. Your sole task is to identify and extract all main authors of a paper from the provided cover page text.\n",
    "\n",
    "=== Core Principles ===\n",
    "- **Find the Block, Then Extract:** Your main goal is to first identify the entire *block* of text containing the author list. Once the block is identified, you MUST extract *all* names from it.\n",
    "- **Signals Validate the Block, Not the Individual:** Superscripts (¹, *) are powerful clues that you have found the correct *group* of names. An individual name does not need a signal to be an author if it is part of that group.\n",
    "\n",
    "=== Step-by-Step Search Strategy ===\n",
    "Follow this process to ensure accuracy:\n",
    "\n",
    "1.  **Locate Anchors:** First, identify the main article **title** and the **abstract**. The author list is almost always located in the space between these two sections.\n",
    "2.  **Identify the Candidate Author Block:** Within that region, search for a block of text that consists of personal names. This block is your primary candidate.\n",
    "3.  **Confirm the Block Using Signals:** Verify that this block is the author list by checking if **at least some of the names** are associated with strong signals:\n",
    "    - **Superscripts:** Numbers (¹, ², ³) or symbols (*, †, ‡) immediately following a name.\n",
    "    - **Affiliation Markers:** Names followed by letters (a, b, c).\n",
    "    - **Corresponding Author Indicators:** An asterisk (*) or the explicit phrase `Corresponding author:`.\n",
    "4.  **Extract ALL Names from the Confirmed Block:** **This is a critical step.** Once you are confident you have the author block, extract every name within it. Do not omit names just because they lack a superscript or other marker.\n",
    "5.  **Apply Exclusion Rules (Crucial):** To avoid errors, you MUST IGNORE names found in the following contexts:\n",
    "    - **In-text Citations:** Any names inside parentheses, e.g., `(Smith et al., 2022)`.\n",
    "    - **References/Bibliography:** Any names in a reference list.\n",
    "    - **Labeled Non-Authors:** Names explicitly labeled as 'Editor', 'Reviewer', or found in sections like 'Acknowledgements'.\n",
    "\n",
    "=== Formatting Rules ===\n",
    "1.  Extract only the full names of the authors.\n",
    "2.  **Remove all extra characters,** including superscript numbers (¹, ², a, *), affiliation markers, degrees, and the word \"and\" before the last author.\n",
    "3.  List all author names, separated by commas.\n",
    "4.  Place the final comma-separated list of names inside an `<authors>` tag.\n",
    "\n",
    "=== Examples ===\n",
    "Input Text:\n",
    "\"A Novel Approach to Machine Learning\n",
    "John A. Smith¹, Jane B. Doe²*, and Michael C. Lee¹\n",
    "¹Department of Computer Science, University of Innovation\n",
    "²Institute for Advanced Studies\"\n",
    "Output:\n",
    "<authors>John A. Smith, Jane B. Doe, Michael C. Lee</authors>\n",
    "\n",
    "Input Text:\n",
    "\"Cellular Mechanisms of Memory Formation\n",
    "ANNA KOWALSKI¹, PIOTR NOWAK, and JANE DOE¹'²*\n",
    "¹Institute of Neuroscience, ²Center for Advanced Brain Studies\n",
    "*Corresponding author: j.doe@email.com\n",
    "Abstract: Memory is a complex process...\"\n",
    "Output:\n",
    "<authors>ANNA KOWALSKI, PIOTR NOWAK, JANE DOE</authors>\n",
    "\n",
    "Input Text:\n",
    "\"Nature Communications | (2025) 16:1234 | https://doi.org/10.1038/s41467-025-12345-x\n",
    "ARTICLE\n",
    "Deep learning for climate model analysis\n",
    "Carlos de la Cruz¹'², Wei Zhang¹*, and Jane Smith³\n",
    "¹Climate Research Institute, ²Department of Physics, ³Data Science Center\n",
    "Abstract: In this paper, we... The work of (Jones, 2021) is relevant...\"\n",
    "Output:\n",
    "<authors>Carlos de la Cruz, Wei Zhang, Jane Smith</authors>\n",
    "\n",
    "=== Instruction ===\n",
    "Analyze the following cover information using the search strategy above. Find the main authors of the article and output ONLY the `<authors>` tag containing their names.\n",
    "\"\"\".strip()\n",
    "prompts = []\n",
    "for i, item in enumerate(cover_information):\n",
    "    article_id, cover_text = item\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_AUTHOR_INFORMATION},\n",
    "        {\"role\": \"user\", \"content\": f\"Cover Information:{cover_text[:2000]}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "print(len(prompts[0]))\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        seed=42,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=96,\n",
    "        temperature=0.1\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "responses = [output.outputs[0].text for output in outputs]\n",
    "\n",
    "author_information = {}\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    author_information[cover_information[i][0]] = response.split(\"</authors>\")[0].replace(\"<authors>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_article_id = '10.1101_2022.07.21.501061'\n",
    "for i, item in enumerate(cover_information):\n",
    "    article_id, cover_text = item\n",
    "    if article_id == test_article_id:\n",
    "        print(cover_text)\n",
    "        # print(author_information[test_article_id])\n",
    "        print(database_information[test_article_id])\n",
    "# 将cover_information转换为DataFrame\n",
    "df_cover = pd.DataFrame(cover_information, columns=['article_id', 'cover_text'])\n",
    "\n",
    "# 保存为CSV文件\n",
    "df_cover.to_csv('/kaggle/working/cover_information.csv', index=False, encoding='utf-8')\n",
    "print(f\"已保存 {len(cover_information)} 条记录到 cover_information.csv\")\n",
    "# print(chunks[0])\n",
    "# for i, item in enumerate(chunks_ref):\n",
    "#     article_id, chunk, _, _ = item\n",
    "#     if article_id == test_article_id:\n",
    "#         print(\"\\n\\n\")\n",
    "#         print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SYS_PROMPT = \"\"\"\n",
    "# You are a highly accurate resource type classifier. Given a snippet of academic text containing a DOI or accession number, you must decide if it points to research data or to literature/code.\n",
    "\n",
    "# Choose one of the following:\n",
    "#   A) Data — The identifier points to a research dataset (e.g., sequencing reads, measurements, images, survey results).\n",
    "#   B) Literature / Code — The identifier points to a journal article, book, preprint, protocol, software, code repository, or computational notebook.\n",
    "\n",
    "# === Step-by-Step Logic ===\n",
    "# 1.  **Check for Publisher Prefixes**: If the DOI prefix belongs to a known journal publisher (e.g., 10.1038, 10.1016, 10.1126, 10.1021, 10.1101), it is always **B**.\n",
    "# 2.  **Check for Data-Specific Repositories**: If the identifier is from a repository that ONLY hosts data (e.g., Dryad `10.5061`, NCBI accessions like `PRJNA`, `SRA`, proteomics DBs like `PXD`), it is always **A**.\n",
    "# 3.  **Check for Context Keywords (for General Repositories)**: For general-purpose repositories like Zenodo (`10.5281`) or Figshare (`10.6084`), you MUST use context words to decide.\n",
    "#     * If you see **Data Keywords** like \"dataset\", \"data\", \"supplementary data\", \"deposited\", \"accession\", choose **A**.\n",
    "#     * If you see **Code/Software Keywords** like \"software\", \"code\", \"scripts\", \"repository\", \"package\", \"pipeline\", \"notebook\", choose **B**.\n",
    "#     * If no specific keywords are present, assume it is data, choose **A**.\n",
    "\n",
    "# === Few-Shot Examples ===\n",
    "# 1) \"Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).\" → A\n",
    "# 2) \"As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).\" → B\n",
    "# 3) \"The analysis **code** is archived on Zenodo (DOI: 10.5281/zenodo.50000).\" → B\n",
    "# 4) \"Sequence reads available under BioProject accession PRJNA765432.\" → A\n",
    "# 5) \"The complete **software package** can be found at 10.5281/zenodo.12345.\" → B\n",
    "# 6) \"Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).\" → B\n",
    "# 7) \"The **dataset** supporting the conclusions is available at 10.6084/m9.figshare.7654321.\" → A\n",
    "# 8) \"Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.\" → B\n",
    "\n",
    "# === Instructions ===\n",
    "# - Follow the logic steps carefully. Context is crucial for general repositories.\n",
    "# - Code and software are NOT data. Classify them as B.\n",
    "# - Output exactly one letter: A or B.\n",
    "# \"\"\".strip()\n",
    "# prompts = []\n",
    "# for i, item in enumerate(chunks):\n",
    "#     article_id, academic_text, dataset_id,_ = item\n",
    "#     # article_id, academic_text = item\n",
    "#     # for ref in items[i]:\n",
    "#     if not dataset_id.startswith(\"https://doi.org/\"):\n",
    "#         continue\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "#         {\"role\": \"user\", \"content\": f\"Text:\\n{academic_text}\\nDOI: {dataset_id}\"}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         tokenize=False,\n",
    "#         enable_thinking=False,\n",
    "#     )\n",
    "#     prompts.append(prompt)\n",
    "# mclp = MultipleChoiceLogitsProcessor(tokenizer, \n",
    "#                                      choices=[\"A\", \"B\"])\n",
    "\n",
    "\n",
    "# outputs = llm.generate(\n",
    "#     prompts,\n",
    "#     vllm.SamplingParams(\n",
    "#         seed=777,\n",
    "#         temperature=0.1,\n",
    "#         skip_special_tokens=True,\n",
    "#         max_tokens=1,\n",
    "#         logits_processors=[mclp],\n",
    "#         logprobs=len(mclp.choices)\n",
    "\n",
    "#     ),\n",
    "#     use_tqdm=True\n",
    "# )\n",
    "# logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "# choices = [max(d, key=d.get) for d in logprobs]\n",
    "# # types = {'A': True, 'B': False}\n",
    "# # choices = [types[c] for c in choices]\n",
    "# idx = 0\n",
    "\n",
    "# for i, item in enumerate(chunks):\n",
    "#     if item[2].startswith(\"https://doi.org/\"):\n",
    "#         if choices[idx] == 'B':\n",
    "#             # print(\"1\")\n",
    "#             answers[i] = None\n",
    "#             idx += 1\n",
    "#         else:\n",
    "#             answers[i] = \"Secondary\"\n",
    "#             idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import collections\n",
    "# print(collections.Counter(choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.DataFrame([\n",
    "    {\n",
    "        'article_id': item[0],\n",
    "        'academic_text': item[1], \n",
    "        'dataset_id': item[2]\n",
    "    }\n",
    "    for item in chunks\n",
    "])\n",
    "\n",
    "# 保存为CSV文件\n",
    "df_chunks.to_csv('/kaggle/working/chunks2.csv', index=False, encoding='utf-8')\n",
    "print(f\"已保存 {len(chunks)} 条记录到 chunks2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYS_PROMPT_ACCESSION = \"\"\"\n",
    "You are a highly strict citation analyst. Your task is to classify an accession ID as Primary (A) or Secondary (B).\n",
    "\n",
    "Choose one of the following:\n",
    " A) Primary — There is conclusive proof the data was generated or submitted as part of the current study.\n",
    " B) Secondary — The data is reused from another source, or there is any ambiguity.\n",
    "\n",
    "=== Input Fields ===\n",
    "1.  **Citation Context**: The local text where the specific `accession ID` appears.\n",
    "2.  **Data Information Context**: A block of text scraped from the paper, which may or may not be the official Data Availability Statement.\n",
    "\n",
    "=== Step-by-Step Decision Logic (Strict Order) ===\n",
    "1.  **Check if \"Data Information Context\" is a Formal Availability Statement.** First, examine the text in the `Data Information Context` field. If this text is clearly a formal Data Availability Statement (e.g., it starts with \"Data Availability:\", \"Availability of data and materials\", or contains phrases like \"data that support the findings of this study are available\"), this is a very strong signal that any related accession IDs are from the current study. In this case, classify the ID as **Primary (A)**. Your decision is final; do not proceed to the next steps.\n",
    "\n",
    "2.  **Adopt a \"Default to Secondary\" Mindset.** If Rule #1 does not apply, assume the `accession ID` is **Secondary (B)** from the start. Your goal is to find *conclusive proof* to overturn this assumption. Ambiguity always results in a Secondary (B) classification.\n",
    "\n",
    "3.  **Search for Explicit Primary Evidence.** You must find a direct, undeniable link between the specific `accession ID` and the current study. Scrutinize both fields for this proof:\n",
    "    -   **Check the `Data Information Context`:** Is the **exact accession ID** (e.g., `PXD006789`) listed? Is it part of a **specific, named range** (e.g., \"accessions ABC001-ABC100\")?\n",
    "    -   **Check the `Citation Context`:** Does the text immediately surrounding the ID contain unambiguous primary keywords? (e.g., \"data generated for this study (accession ID)\", \"we submitted our sequences, including accession ID\").\n",
    "\n",
    "4.  **Make the Final Classification:**\n",
    "    -   If you found explicit proof in Step 3, classify as **Primary (A)**.\n",
    "    -   If the evidence is merely circumstantial (e.g., \"the ID is a GenBank ID and the authors mention submitting other data to GenBank\") this is **NOT sufficient proof**. You must classify it as **Secondary (B)**.\n",
    "\n",
    "=== Few-Shot Examples ===\n",
    "1) **Case: New Rule #1 in action.**\n",
    "    - `Citation Context`: \"The resulting sequences were grouped into OTUs (see OTU_345 in Table S1).\" (Neutral)\n",
    "    - `Data Information Context`: \"Data Availability: All sequence data generated for this study have been deposited in the GenBank database under accession numbers ON12345-ON12400.\"\n",
    "    - → **A** (Reasoning: The `Data Information Context` is a formal Data Availability Statement describing primary data submission. Rule #1 applies, so the ID is classified as Primary, even though the specific ID 'OTU_345' isn't listed.)\n",
    "\n",
    "2) **Case: Stricter logic applies when Rule #1 fails.**\n",
    "    - `Citation Context`: \"Table 1 shows the list of isolates, including EPI_ISL_291131.\" (Neutral)\n",
    "    - `Data Information Context`: \"The whole-genome sequences of A(H7N4) isolates in this study were submitted to the Global Initiative on Sharing All Influenza Data (GISAID...) (accession numbers EPI_ISL_445001 to EPI_ISL_445093).\"\n",
    "    - → **A** (Reasoning: Rule #1 applies because the context is a formal data submission statement for \"isolates in this study.\" Therefore, `EPI_ISL_291131`, which is an isolate, is considered Primary.)\n",
    "    \n",
    "3) **Case: Clear secondary data reuse.**\n",
    "    - `Citation Context`: \"We **retrieved** patient gene expression data from the TCGA portal (dbGaP accession phs000178) for our analysis.\" (Clearly Secondary)\n",
    "    - `Data Information Context`: \"All custom scripts generated for this study are available on GitHub.\"\n",
    "    - → **B** (Reasoning: The `Data Information Context` is about code, not the data in question. The \"Default to Secondary\" logic applies, and the context keyword \"retrieved\" confirms it.)\n",
    "\n",
    "4) **Case: Clear primary data submission (explicit proof found).**\n",
    "    - `Citation Context`: \"The proteomics data can be found in the PRIDE archive: PXD006789.\" (Neutral)\n",
    "    - `Data Information Context`: \"The mass spectrometry proteomics data **have been deposited** to the ProteomeXchange Consortium via the PRIDE partner repository with the dataset identifier PXD006789.\"\n",
    "    - → **A** (Reasoning: The `Data Information Context` is a formal DAS (Rule #1). It also contains the *exact* accession ID, providing conclusive proof.)\n",
    "\n",
    "=== Instructions ===\n",
    "- Follow the decision logic strictly in order. Rule #1 is a shortcut; if it applies, your decision is made.\n",
    "- If Rule #1 does not apply, be strict and default to Secondary (B) unless there is explicit proof.\n",
    "- Output exactly one letter: A or B.\n",
    "\"\"\".strip()\n",
    "prompts = []\n",
    "primary_reg = CLASSIFICATION[\"primary\"]\n",
    "tasks_to_process = []\n",
    "\n",
    "for i, item in enumerate(chunks):\n",
    "    article_id, academic_text, dataset_id,_ = item\n",
    "    # article_id, academic_text = item\n",
    "    # for ref in items[i]:\n",
    "    # if not dataset_id.startswith(\"EPI_ISL_291131\"):\n",
    "    #     continue\n",
    "    if dataset_id.startswith(\"https://doi.org/\") or not answers[i] or dataset_id.startswith(\"EPI_ISL_\"):\n",
    "        continue\n",
    "    elif not primary_reg.search(academic_text):\n",
    "        continue\n",
    "    # elif not primary_reg.search(academic_text) and (article_id, dataset_id) in tasks_to_process:\n",
    "    #     continue\n",
    "    tasks_to_process.append((article_id, dataset_id))\n",
    "    data_information = article_data_information[article_id]\n",
    "    if data_information:\n",
    "        # print(data_information)\n",
    "        data_information = ''.join(data_information)[:1000]\n",
    "    else:\n",
    "        # print(article_id)\n",
    "        data_information = \"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n",
    "        {\"role\": \"user\", \"content\": f\"Text:\\n\\n{academic_text}\\n\\n\\nData information:{data_information}\\n\\n\\nThe prediction result for accession ID {dataset_id} is:\"}\n",
    "    ]\n",
    "    # print(messages[1][\"content\"])\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "print(len(prompts[0]))\n",
    "mclp = MultipleChoiceLogitsProcessor(tokenizer, \n",
    "                                     choices=[\"A\", \"B\", \"C\"])\n",
    "\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        seed=777,\n",
    "        temperature=0.01,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[mclp],\n",
    "        logprobs=len(mclp.choices)\n",
    "\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "choices = [max(d, key=d.get) for d in logprobs]\n",
    "\n",
    "idx = 0\n",
    "tasks_to_process = []\n",
    "for i, item in enumerate(chunks):\n",
    "    article_id, academic_text, dataset_id,_ = item\n",
    "    if not dataset_id.startswith(\"https://doi.org/\") and answers[i] and not dataset_id.startswith(\"EPI_ISL_\"):\n",
    "        if not primary_reg.search(academic_text):\n",
    "            continue\n",
    "        # if not primary_reg.search(academic_text) and (article_id, dataset_id) in tasks_to_process:\n",
    "        #     continue\n",
    "        tasks_to_process.append((article_id, dataset_id))\n",
    "        if choices[idx] == 'A':\n",
    "            answers[i] = \"Primary\"\n",
    "        elif choices[idx] == 'B':\n",
    "            answers[i] = \"Secondary\"\n",
    "        else:\n",
    "            answers[i] = None  # 低置信度/无效情况\n",
    "        idx += 1   \n",
    "        \n",
    "        \n",
    "# # 设定比例权重（可调参数）\n",
    "# A_weight = 10  # 降低Primary的选择概率\n",
    "# B_weight = 1  # 提高Secondary的选择概率\n",
    "# C_weight = 2000000000  # 大幅降低低置信度的选择概率\n",
    "\n",
    "# choices = []\n",
    "# for d in logprobs:\n",
    "#     prob_a = d.get('A', -float('inf')) * A_weight  # 加权后的概率\n",
    "#     prob_b = d.get('B', -float('inf')) * B_weight\n",
    "#     prob_c = d.get('C', -float('inf')) * C_weight  # 若无C类，默认忽略\n",
    "    \n",
    "#     # 选择调整后概率最高的类别\n",
    "#     adjusted_probs = {'A': prob_a, 'B': prob_b, 'C': prob_c}\n",
    "#     choice = max(adjusted_probs.items(), key=lambda x: x[1])[0]\n",
    "#     choices.append(choice)\n",
    "\n",
    "# # 结果分配\n",
    "# idx = 0\n",
    "# for i, item in enumerate(chunks):\n",
    "#     if not item[2].startswith(\"https://doi.org/\"):\n",
    "#         if choices[idx] == 'A':\n",
    "#             answers[i] = \"Primary\"\n",
    "#         elif choices[idx] == 'B':\n",
    "#             answers[i] = \"Secondary\"\n",
    "#         else:\n",
    "#             answers[i] = None  # 低置信度/无效\n",
    "#         idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(choices)\n",
    "# print(article_data_information[\"10.1128_JVI.01717-21\"])\n",
    "# # print(outputs)\n",
    "# # print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "You are an expert citation analyst. Your task is to classify data citations as either Primary or Secondary using a combination of author matching and contextual clues.\n",
    "\n",
    "You will be given the text containing the citation and a list of the paper's authors.\n",
    "\n",
    "Choose one of the following:\n",
    " A) Primary — The data was generated for the current study, or is the authors' own data from a prior study. A match between the cited authors and the paper's authors is the strongest signal.\n",
    " B) Secondary — The data was retrieved, reused, or re-analyzed from an external source created by a different research group.\n",
    "\n",
    "=== Decision-Making Process ===\n",
    "Follow this strict order. A rule's conclusion is final.\n",
    "\n",
    "1.  **Prioritize Author Match.** This is the most reliable signal. If the citation text includes author names (e.g., \"from Smith et al., 2022\"), compare them to the provided list of the paper's authors.\n",
    "    * If the authors **match**, classify as **PRIMARY (A)**. An author match overrides any conflicting contextual keywords (e.g., \"re-analyzed our previous data\").\n",
    "    * If the authors **do not match**, classify as **SECONDARY (B)**.\n",
    "\n",
    "2.  **Use Contextual Keywords as a Fallback.** If the citation text does **NOT** include any author names (e.g., it only contains an accession number or DOI), then you must rely on the contextual keywords to make your decision.\n",
    "    * **Primary Keywords (A):** \"data have been deposited\", \"generated in this study\", \"are available in\", \"data supporting this article\", \"submitted to\", \"data generated for this work\".\n",
    "    * **Secondary Keywords (B):** \"data were downloaded from\", \"obtained from\", \"retrieved from\", \"previously described in\", \"publicly available dataset from\", \"we re-analyzed the data from\".\n",
    "\n",
    "=== Author Matching Guidelines ===\n",
    "- A match occurs if the first author in an \"et al.\" citation is one of the paper's authors.\n",
    "- A match occurs if the cited authors are a subset of the paper's authors, or vice-versa.\n",
    "- The last name must match exactly, and initials in one name must be a subset of the other (e.g., \"J. Smith\" matches \"Jane F. Smith\").\n",
    "\n",
    "=== Few-Shot Examples ===\n",
    "1) Text: \"We re-analyzed the microarray data previously published by **Smith et al.** (GEO: GSE54321).\"\n",
    "   Paper Authors: **Smith J**, Miller K, Chu L\n",
    "   → A (Author match is the priority, indicating they are re-analyzing their own prior work.)\n",
    "\n",
    "2) Text: \"We re-analyzed the microarray data previously published by **Smith et al.** (GEO: GSE54321).\"\n",
    "   Paper Authors: Jones B, Williams R, Chen F\n",
    "   → B (Authors do not match, confirming reuse of external data.)\n",
    "\n",
    "3) Text: \"All raw sequencing data **have been deposited** in the SRA database under accession SRP123456.\"\n",
    "   Paper Authors: Garcia L, Davis A\n",
    "   → A (No cited author in the text, so the decision falls back to the primary keyword \"have been deposited\".)\n",
    "\n",
    "4) Text: \"We **retrieved** patient gene expression data from the TCGA portal (dbGaP accession phs000178).\"\n",
    "   Paper Authors: Rodriguez H, Kim P\n",
    "   → B (No cited author in the text, so the decision falls back to the secondary keyword \"retrieved\".)\n",
    "\n",
    "5) Text: \"Data was obtained from the UK Biobank resource under application number 12345.\"\n",
    "   Paper Authors: Thompson S, Baker M\n",
    "   → B (No cited author, fallback to secondary keyword \"obtained from\".)\n",
    "\n",
    "6) Text: \"The full dataset **supporting the conclusions of this article is available** in the Figshare repository [DOI].\"\n",
    "   Paper Authors: Clark D, Moore T\n",
    "   → A (No cited author, fallback to primary keyword phrase.)\n",
    "\n",
    "=== Instructions ===\n",
    "- First, check for an author match between the citation text and the paper's author list. This is your primary decision tool.\n",
    "- If and only if no author is mentioned in the citation text, use the contextual keywords.\n",
    "- Output exactly one letter: A or B.\n",
    "\"\"\".strip()\n",
    "prompts = []\n",
    "for i in range(body_length):\n",
    "    article_id, academic_text, dataset_id,_ = chunks[i]\n",
    "    # article_id, academic_text = item\n",
    "    # for ref in items[i]:\n",
    "    if not dataset_id.startswith(\"https://doi.org/\") or not answers[i]:\n",
    "        continue\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n",
    "        {\"role\": \"user\", \"content\": f\"Text:\\n{academic_text}\\nPaper Authors:{author_information[article_id]}\\nDOI: {dataset_id}\\nResult is:\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "print(len(prompts[0]))\n",
    "mclp = MultipleChoiceLogitsProcessor(tokenizer, \n",
    "                                     choices=[\"A\", \"B\"])\n",
    "\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        seed=777,\n",
    "        temperature=0.1,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[mclp],\n",
    "        logprobs=len(mclp.choices)\n",
    "\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "choices = [max(d, key=d.get) for d in logprobs]\n",
    "# types = {'A': True, 'B': False}\n",
    "# choices = [types[c] for c in choices]\n",
    "idx = 0\n",
    "for i in range(body_length):\n",
    "    if not chunks[i][2].startswith(\"https://doi.org/\") or not answers[i]:\n",
    "        continue\n",
    "    if choices[idx] == 'A':\n",
    "        # print(\"1\")\n",
    "        answers[i] = \"Primary\"\n",
    "        idx += 1\n",
    "    else:\n",
    "        answers[i] = \"Secondary\"\n",
    "        idx += 1\n",
    "# for i, item in enumerate(chunks_ref):\n",
    "#     if choice == \"A\":\n",
    "#         answers[body_length + i] = \"Primary\"\n",
    "\n",
    "# for i, item in enumerate(chunks_ref):\n",
    "#     if item[2].startswith(\"https://doi.org/\") and answers[i] and choices[idx] == 'B':\n",
    "#         answers[i] = None\n",
    "#         idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(cover_information[:2000])\n",
    "# print(author_information)\n",
    "# print(len(chunks))\n",
    "# print(len(answers))\n",
    "# print(answers)\n",
    "# print(chunks_ref)\n",
    "# print(len(chunks))\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYS_PROMPT_CLASSIFY_CITATION_TYPE = \"\"\"\n",
    "You are a highly accurate citation type classifier. Your task is to determine if a cited dataset is Primary (A - generated for this study) or Secondary (B - reused from another study).\n",
    "\n",
    "=== Provided Information ===\n",
    "- **Target Identifier:** The specific DOI or accession number you must classify.\n",
    "- **Reference Context:** A block of text from the paper's bibliography. **This may contain multiple reference entries.**\n",
    "- **Paper Authors:** The author list of the paper being analyzed.\n",
    "- **Data Availability:** Additional context scraped from the paper's dedicated data availability section.\n",
    "\n",
    "=== Decision-Making Process (Strict Order) ===\n",
    "1.  **Isolate the Target Citation FIRST.** This is your most critical step. Locate the `Target Identifier` within the `Reference Context`. Identify the single, complete reference entry that contains this identifier.\n",
    "    -   **Crucially, you must IGNORE all other reference entries in the context.**\n",
    "    -   All subsequent steps apply **ONLY** to this isolated target citation.\n",
    "\n",
    "2.  **Formal Section Check:** If the isolated citation is a formal statement like \"Data Availability: All data are available...\", it is **PRIMARY (A)**. This rule is absolute.\n",
    "\n",
    "3.  **Author Match Check:** This is the most important signal. Compare the authors of the **isolated target citation** to the **Paper Authors** using the flexible rules in the **`Author Matching Guidelines`** below.\n",
    "    -   If a **strong overlap** exists, the citation is **PRIMARY (A)**. This overrules all other context.\n",
    "    -   A strong overlap exists if **at least one author** from the isolated citation can be matched to an author in the `Paper Authors` list.\n",
    "    -   If there is no overlap, it is **SECONDARY (B)**.\n",
    "\n",
    "4.  **Handle Institutional/Ambiguous Authors (Default to Primary):** If the isolated citation has no matching personal authors (e.g., \"GBIF.org\"), or is ambiguous, analyze the **Data Availability** field:\n",
    "    -   If the `Data Availability` field contains explicit **secondary** keywords (e.g., \"downloaded from\", \"reused\"), classify as **SECONDARY (B)**.\n",
    "    -   Otherwise (the field is empty, contains primary keywords, etc.), classify as **PRIMARY (A)**.\n",
    "\n",
    "5.  **Context Keyword Check (If No Authors):** If the previous rules do not apply, use keywords within the isolated citation to decide:\n",
    "    -   **PRIMARY (A)** keywords: \"Data from this study\", \"Data underlying this paper\".\n",
    "    -   **SECONDARY (B)** keywords: \"Data from [Author et al. 2019]\", \"Publicly available data from...\".\n",
    "\n",
    "6.  **True \"Naked\" Citation Rule:** If the isolated citation is just the `Target Identifier` and the `Data Availability` field is empty, it is **PRIMARY (A)**.\n",
    "\n",
    "=== Author Matching Guidelines ===\n",
    "- **Matching is flexible:** The goal is to determine if the cited author is among the paper's authors, even if name formatting differs.\n",
    "- **Last Name is Key:** The last name must match exactly.\n",
    "- **Order is Ignored:** `Koovsk, P. M.` is considered identical to `P. M. Koovsk` or `Patrick M. Koovsk`. The model must identify the last name regardless of position.\n",
    "- **Initials must be a Subset:** A name with initials (e.g., `P. Koovsk`) matches a full name (e.g., `Patrick M. Koovsk`) if the last name is identical and the initials in the citation (`P`) are a subset of the full name's initials (`P`, `M`).\n",
    "- **\"et al.\" Rule:** If the first author in an \"et al.\" citation matches one of the paper's authors, it is a strong overlap.\n",
    "\n",
    "=== Key Examples ===\n",
    "1.  **Case: Correctly isolating the target citation (Fixed Bad Case).**\n",
    "    - `Target Identifier`: `https://doi.org/10.11588/data/10100`\n",
    "    - `Reference Context`: \"Hajdas I...2019...Radiocarbon 61(5):11331134.\\nHammer S, Levin I. 2017. Monthly mean atmospheric D14CO2... https://doi.org/10.11588/data/10100 heiDATA...\\nHandlos P...\"\n",
    "    - `Paper Authors`: \"G Quarta, I Hajdas, M Molnr, T Varga...\"\n",
    "    - `Data Availability`: \"\"\n",
    "    - → **B** (Reasoning: First, isolate the \"Hammer S, Levin I\" entry. Then, compare its authors to the Paper Authors. Per the guidelines, there is no name match. Therefore, it is Secondary.)\n",
    "\n",
    "2.  **Case: Author Match with different formatting.**\n",
    "    - `Target Identifier`: `GSE12345`\n",
    "    - `Reference Context`: \"Koovsk, P. M. & Smith, A. (2022) RNA-seq data... Gene Expression Omnibus, GSE12345.\"\n",
    "    - `Paper Authors`: \"Anna Smith, Patrick M. Koovsk\"\n",
    "    - `Data Availability`: \"\"\n",
    "    - → **A** (Reasoning: The isolated entry's authors \"Koovsk, P. M.\" and \"Smith, A.\" are matched to \"Patrick M. Koovsk\" and \"Anna Smith\" using the flexible matching guidelines.)\n",
    "\n",
    "=== Instructions ===\n",
    "- Follow the **Decision-Making Process** strictly, starting with isolating the correct reference.\n",
    "- Use the detailed **Author Matching Guidelines** to determine if an overlap exists.\n",
    "- Output exactly one letter: A or B.\n",
    "\"\"\".strip()\n",
    "prompts = []\n",
    "for i, item in enumerate(chunks_ref):\n",
    "    article_id, academic_text, dataset_id,patten = item\n",
    "    if not answers[body_length + i]:\n",
    "        continue\n",
    "    # if not dataset_id.startswith(\"https://doi.org/10.5066/p9gtumay\"):\n",
    "    #     continue\n",
    "    data_information = article_data_information[article_id]\n",
    "    if data_information:\n",
    "        data_information = '\\n'.join(data_information)\n",
    "    else:\n",
    "        data_information = \"\"\n",
    "    if dataset_id.startswith(\"https://doi.org/\"):\n",
    "        doi_part = dataset_id.removeprefix(\"https://doi.org/\")\n",
    "        prefix = doi_part.split('/')[0]\n",
    "        article_data = database_information.get(article_id, {})\n",
    "        context_list = article_data.get(prefix, [])\n",
    "        database_info = ''.join(context_list)\n",
    "        data_information = database_info + data_information\n",
    "    data_information = data_information[:1000]\n",
    "    author = author_information[article_id]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_CITATION_TYPE},\n",
    "        {\"role\": \"user\", \"content\": f\"Text:\\n{academic_text}\\n\\nTarget Identifier: {dataset_id}\\n\\nPaper Authors:{author}\\n\\nData Availability:\\n{data_information}\\nResult is:\"}\n",
    "    ]\n",
    "    # print(messages[1][\"content\"])\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "print(len(prompts[0]))\n",
    "mclp = MultipleChoiceLogitsProcessor(tokenizer, \n",
    "                                     choices=[\"A\", \"B\"])\n",
    "\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        seed=777,\n",
    "        temperature=0.01,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[mclp],\n",
    "        logprobs=len(mclp.choices)\n",
    "\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "# print(outputs)\n",
    "logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "choices = [max(d, key=d.get) for d in logprobs]\n",
    "# types = {'A': True, 'B': False}\n",
    "# choices = [types[c] for c in choices]\n",
    "idx = 0\n",
    "for i, item in enumerate(chunks_ref):\n",
    "    if not answers[body_length + i]:\n",
    "        continue\n",
    "    if choices[idx] == 'A':\n",
    "        # print(\"1\")\n",
    "        answers[body_length + i] = \"Primary\"\n",
    "        idx += 1\n",
    "    else:\n",
    "        answers[body_length + i] = \"Secondary\"\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-29T00:18:31.624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PAPER_PREFIXES = [\n",
    "#     \"10.1007\", \"10.1002\", \"10.1016\", \"10.1021\", \"10.1038\", \"10.1056\",\n",
    "#     \"10.1073\", \"10.1080\", \"10.1093\", \"10.1101\", \"10.1186\", \"10.1371\",\n",
    "#     \"10.1111\", \"10.5194\", \"10.3390\", \"10.1126\", \"10.1103\", \"10.1210\"\n",
    "# ]\n",
    "\n",
    "# CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "# def is_paper_prefix(doi_string):\n",
    "#     core_doi = doi_string[len(\"https://doi.org/\"):]\n",
    "#     for prefix in PAPER_PREFIXES:\n",
    "#         if core_doi.startswith(prefix):\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def has_context_keyword(context_string):\n",
    "#     if not isinstance(context_string, str):\n",
    "#         return False\n",
    "#     return bool(re.search(CONTEXT_RE, context_string))\n",
    "# for i, item in enumerate(chunks):\n",
    "#     article_id, academic_text, dataset_id,_ = item\n",
    "#     if dataset_id.startswith(\"https://doi.org/\") and answers[i]:\n",
    "#         if is_paper_prefix(dataset_id) and not has_context_keyword(academic_text):\n",
    "#             answers[i] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:10:30.682444Z",
     "iopub.status.busy": "2025-08-29T08:10:30.682044Z",
     "iopub.status.idle": "2025-08-29T08:10:30.779906Z",
     "shell.execute_reply": "2025-08-29T08:10:30.779164Z",
     "shell.execute_reply.started": "2025-08-29T08:10:30.682417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"article_id\"] = [c[0] for c in chunks]\n",
    "sub_df[\"chunk\"] = [c[1].replace(r'\"',\"'\") for c in chunks]\n",
    "sub_df[\"pattern_name\"] = [c[3] for c in chunks]\n",
    "sub_df[\"dataset_id\"] = [c[2] for c in chunks]\n",
    "# sub_df[\"dataset_id\"] = sub_df[\"dataset_id\"].str.lower()\n",
    "sub_df[\"type\"] = answers\n",
    "sub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_doi_mask = ~sub_df[\"dataset_id\"].str.startswith(\"https://doi.org/\", na=False)\n",
    "primary_non_doi = sub_df[non_doi_mask & (sub_df[\"type\"] == \"Primary\")]\n",
    "# 按照 article_id 和 pattern_name 分组，只处理包含超过两个\"Primary\"的组\n",
    "grouped = primary_non_doi.groupby([\"article_id\", \"pattern_name\"])\n",
    "for (article_id, pattern_name), group in grouped:\n",
    "    if pattern_name in ['ena_accession']:\n",
    "        continue\n",
    "    if len(group) >= 10:  # 只有当同一个组中有2个或更多Primary时才进行处理\n",
    "        same_group_mask = (sub_df[\"article_id\"] == article_id) & (sub_df[\"pattern_name\"] == pattern_name)\n",
    "        sub_df.loc[same_group_mask, \"type\"] = \"Primary\"\n",
    "# new_rows = []\n",
    "# existing_article_ids = set(sub_df[\"article_id\"].unique())\n",
    "# for article_id, dataset_ids in article_data.items():\n",
    "#     # 检查 article_id 是否在 sub_df 中存在\n",
    "#     if article_id.replace(\"/\",\"_\") in existing_article_ids:\n",
    "#         # 如果存在，则遍历其关联的所有 dataset_id\n",
    "#         for dataset_id in dataset_ids:\n",
    "#             # 根据规则确定 type\n",
    "#             row_type = \"\"\n",
    "#             if dataset_id.startswith(\"10.\"):\n",
    "#                 # continue\n",
    "#                 row_type = \"Primary\"\n",
    "#             # elif dataset_id.startswith(\"SAMN\"):\n",
    "#             #     row_type = \"Primary\"\n",
    "#             # else:\n",
    "#             #     row_type = \"Secondary\"\n",
    "#             # 创建新行的数据字典\n",
    "#             else:\n",
    "#                 continue\n",
    "#             new_row = {\n",
    "#                 \"article_id\": article_id.replace(\"/\", \"_\"),\n",
    "#                 \"chunk\": \"\",  # chunk值设为空\n",
    "#                 \"dataset_id\": \"https://doi.org/\" + dataset_id if dataset_id.startswith(\"10.\") else dataset_id,\n",
    "#                 \"type\": row_type\n",
    "#             }\n",
    "#             new_rows.append(new_row)\n",
    "# if new_rows:\n",
    "#     # 将新行列表转换为 DataFrame\n",
    "#     new_rows_df = pd.DataFrame(new_rows)\n",
    "#     # 使用 pd.concat 将新行合并到原始 DataFrame\n",
    "#     sub_df = pd.concat([sub_df, new_rows_df], ignore_index=True)\n",
    "# sub_df = sub_df.drop_duplicates(\n",
    "#     subset=['article_id', 'dataset_id'], keep=\"first\"\n",
    "# ).reset_index(drop=True)\n",
    "# sub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # 1. 分割数据：doi_df 和 accession_df\n",
    "# doi_df = sub_df[sub_df[\"dataset_id\"].str.startswith(\"https://doi.org/\", na=False)]\n",
    "# accession_df = sub_df[~sub_df[\"dataset_id\"].str.startswith(\"https://doi.org/\", na=False)]\n",
    "\n",
    "# # 2. 找出 accession_df 中 article_id 也存在于 doi_df 的行，并删除\n",
    "# duplicate_article_ids = set(doi_df[\"article_id\"]).intersection(set(accession_df[\"article_id\"]))\n",
    "# accession_df_cleaned = accession_df[~accession_df[\"article_id\"].isin(duplicate_article_ids)]\n",
    "\n",
    "# # 3. 合并 doi_df 和 accession_df_cleaned\n",
    "# sub_df = pd.concat([doi_df, accession_df_cleaned], ignore_index=True)\n",
    "\n",
    "\n",
    "sub_df = sub_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=True).drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "sub_df['row_id'] = range(len(sub_df))\n",
    "sub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"article_id\", \"dataset_id\", \"type\"])\n",
    "sub_df.to_csv(\"submission_chunk.csv\", escapechar='\\\\', quoting=csv.QUOTE_ALL, index=False, columns=[\"row_id\", \"chunk\", \"article_id\", \"dataset_id\", \"type\"])\n",
    "sub_df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:10:35.267039Z",
     "iopub.status.busy": "2025-08-29T08:10:35.266716Z",
     "iopub.status.idle": "2025-08-29T08:10:35.382362Z",
     "shell.execute_reply": "2025-08-29T08:10:35.381239Z",
     "shell.execute_reply.started": "2025-08-29T08:10:35.267016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    \"\"\"Calculates the F1 score.\"\"\"\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "\n",
    "def evaluate_and_print(pred_df, label_df, category_name=\"Overall\"):\n",
    "    \"\"\"\n",
    "    Calculates and prints performance metrics, including the top 10 false positives.\n",
    "    \"\"\"\n",
    "    if pred_df.empty and label_df.empty:\n",
    "        print(f\"--- No data for {category_name} ---\")\n",
    "        return\n",
    "\n",
    "    # 使用 outer merge 并带指示器来高效地找到 TP, FP, FN\n",
    "    # _merge 列会告诉我们每一行数据的来源:\n",
    "    # 'both' -> 左右都有 (TP)\n",
    "    # 'right_only' -> 只在右边 (pred_df) 有 (FP)\n",
    "    # 'left_only' -> 只在左边 (label_df) 有 (FN)\n",
    "    merged_df = pd.merge(\n",
    "        label_df,\n",
    "        pred_df,\n",
    "        on=[\"article_id\", \"dataset_id\", \"type\"],\n",
    "        how='outer',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # 1. 真正例 (True Positives, TP)\n",
    "    tp = (merged_df['_merge'] == 'both').sum()\n",
    "\n",
    "    # 2. 假正例 (False Positives, FP)\n",
    "    fp_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=['_merge'])\n",
    "    fp = fp_df.shape[0]\n",
    "\n",
    "    # 3. 假反例 (False Negatives, FN)\n",
    "    fn_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    fn = fn_df.shape[0]\n",
    "    \n",
    "    # 精确率 (Precision)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    # 召回率 (Recall)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # F1 分数\n",
    "    f1 = f1_score(tp, fp, fn)\n",
    "\n",
    "    # --- 开始格式化输出 ---\n",
    "    print(f\"\\n--- {category_name} Performance Metrics ---\")\n",
    "    print(\"--- 核心指标 ---\")\n",
    "    print(f\"TP (真正例): {tp:<5}\")\n",
    "    print(f\"FP (假正例): {fp:<5}\")\n",
    "    print(f\"FN (假反例): {fn:<5}\")\n",
    "    print(\"\\n--- 性能评估 ---\")\n",
    "    print(f\"精确率 (Precision): {precision:.2%}\")\n",
    "    print(f\"召回率 (Recall)    : {recall:.2%}\")\n",
    "    print(f\"F1 Score         : {f1:.3f}\")\n",
    "\n",
    "    # ============================================\n",
    "    # VVVV 这里是新增的功能 VVVV\n",
    "    # ============================================\n",
    "    if fp > 0:\n",
    "        print(\"\\n--- 前10个假正例 (False Positives) 示例 ---\")\n",
    "        # 使用 .to_string() 保证DataFrame能够完整打印不被截断\n",
    "        print(fp_df.head(300).to_string())\n",
    "    else:\n",
    "        print(\"\\n--- 没有发现假正例 (False Positives) ---\")\n",
    "    # if fn > 0:\n",
    "    #     print(\"\\n--- 前10个假正例 (False Positives) 示例 ---\")\n",
    "    #     # 使用 .to_string() 保证DataFrame能够完整打印不被截断\n",
    "    #     print(fn_df.head(100).to_string())\n",
    "    # else:\n",
    "    #     print(\"\\n--- 没有发现假正例 (False Positives) ---\")\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    try:\n",
    "        # Define the path to the labels file.\n",
    "        # Ensure 'train_labels.csv' is in the same directory or provide the correct path.\n",
    "        # labels_dir = labels_dir\n",
    "        \n",
    "        pred_df = pd.read_csv(\"submission.csv\")\n",
    "        \n",
    "        label_df = pd.read_csv(labels_dir)\n",
    "        label_df_missing = label_df[label_df['type'] == 'Missing'].reset_index(drop=True)\n",
    "        label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "        pred_df = pred_df[~pred_df['article_id'].isin(label_df_missing['article_id'])]\n",
    "        # --- Categorize data into DOI and Accession ID ---\n",
    "        \n",
    "        # For prediction data\n",
    "        is_doi_pred = pred_df['dataset_id'].str.startswith(\"https://doi.org/\")\n",
    "        doi_pred_df = pred_df[is_doi_pred]\n",
    "        accession_pred_df = pred_df[~is_doi_pred]\n",
    "        \n",
    "        # For label data\n",
    "        is_doi_label = label_df['dataset_id'].str.startswith(\"https://doi.org/\")\n",
    "        doi_label_df = label_df[is_doi_label]\n",
    "        accession_label_df = label_df[~is_doi_label]\n",
    "        \n",
    "        # --- Evaluate each category ---\n",
    "        \n",
    "        # 1. DOI\n",
    "        evaluate_and_print(doi_pred_df, doi_label_df, category_name=\"DOI\")\n",
    "        \n",
    "        # 2. Accession ID\n",
    "        evaluate_and_print(accession_pred_df, accession_label_df, category_name=\"Accession ID\")\n",
    "        \n",
    "        # 3. Overall (All types)\n",
    "        evaluate_and_print(pred_df, label_df, category_name=\"Overall (All)\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"错误：无法找到 'submission.csv' 或 'train_labels.csv' 文件。请检查文件路径是否正确。\")\n",
    "    except Exception as e:\n",
    "        print(f\"在评估过程中发生了一个错误: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "datasetId": 7713886,
     "isSourceIdPinned": true,
     "sourceId": 12398383,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7789892,
     "sourceId": 12602505,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7958585,
     "isSourceIdPinned": false,
     "sourceId": 12622485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7897334,
     "sourceId": 12711835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8049176,
     "sourceId": 12738413,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8055622,
     "sourceId": 12743408,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8034343,
     "sourceId": 12758803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8139859,
     "sourceId": 12868117,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8143842,
     "sourceId": 12873793,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141469,
     "sourceId": 166258,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141558,
     "sourceId": 166361,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301511,
     "sourceId": 363131,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
